{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quote Data Collection and Processing Pipeline\n",
    "\n",
    "This notebook implements the data collection and processing pipeline according to specs/data_collection_processing_v2.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from datasets import load_dataset\n",
    "import difflib\n",
    "import re\n",
    "import os\n",
    "from typing import List, Dict, Tuple\n",
    "import numpy as np\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Configuration\nKAGGLE_CSV_PATH = \"../data/inspirational_quotes_kaggle.csv\"\nHUGGINGFACE_DATASET = \"asuender/motivational-quotes\"\nGITHUB_GIST_URL = \"https://gist.githubusercontent.com/JakubPetriska/060958fd744ca34f099e947cd080b540/raw/963b5a9355f04741239407320ac973a6096cd7b6/quotes.csv\"\nOUTPUT_PATH = \"../data/processed/unified_quotes_dataset.csv\"\n\n# Create output directory\nPath(\"../data/processed\").mkdir(parents=True, exist_ok=True)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 1: Data Ingestion & Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_kaggle_data() -> pd.DataFrame:\n",
    "    \"\"\"Load Kaggle inspirational quotes dataset\"\"\"\n",
    "    df = pd.read_csv(KAGGLE_CSV_PATH)\n",
    "    \n",
    "    # Standardize column names and add metadata\n",
    "    df_standard = pd.DataFrame({\n",
    "        'quote_text': df['quote'],\n",
    "        'author': df['author'],\n",
    "        'source_dataset': 'kaggle',\n",
    "        'instruction_prompt': '',\n",
    "        'existing_categories': df['category']\n",
    "    })\n",
    "    \n",
    "    print(f\"Loaded {len(df_standard)} quotes from Kaggle dataset\")\n",
    "    return df_standard\n",
    "\n",
    "def load_huggingface_data() -> pd.DataFrame:\n",
    "    \"\"\"Load HuggingFace motivational quotes dataset\"\"\"\n",
    "    dataset = load_dataset(HUGGINGFACE_DATASET, \"quotes_extended\", split=\"train\")\n",
    "    df = dataset.to_pandas()\n",
    "    \n",
    "    # Standardize column names and add metadata\n",
    "    df_standard = pd.DataFrame({\n",
    "        'quote_text': df['quote'],\n",
    "        'author': df['author'],\n",
    "        'source_dataset': 'huggingface',\n",
    "        'instruction_prompt': df['prompt'],\n",
    "        'existing_categories': ''\n",
    "    })\n",
    "    \n",
    "    print(f\"Loaded {len(df_standard)} quotes from HuggingFace dataset\")\n",
    "    return df_standard\n",
    "\n",
    "def load_github_data() -> pd.DataFrame:\n",
    "    \"\"\"Load GitHub gist quotes dataset\"\"\"\n",
    "    response = requests.get(GITHUB_GIST_URL)\n",
    "    \n",
    "    # Save to temporary file and read with pandas\n",
    "    with open(\"temp_github_quotes.csv\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(response.text)\n",
    "    \n",
    "    df = pd.read_csv(\"temp_github_quotes.csv\")\n",
    "    os.remove(\"temp_github_quotes.csv\")\n",
    "    \n",
    "    # Standardize column names and add metadata\n",
    "    df_standard = pd.DataFrame({\n",
    "        'quote_text': df['Quote'],\n",
    "        'author': df['Author'],\n",
    "        'source_dataset': 'github',\n",
    "        'instruction_prompt': '',\n",
    "        'existing_categories': ''\n",
    "    })\n",
    "    \n",
    "    print(f\"Loaded {len(df_standard)} quotes from GitHub gist\")\n",
    "    return df_standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 29296 quotes from Kaggle dataset\n",
      "Loaded 4262 quotes from HuggingFace dataset\n",
      "Loaded 1664 quotes from GitHub gist\n",
      "\n",
      "Total quotes before processing: 35222\n",
      "Source distribution:\n",
      "source_dataset\n",
      "kaggle         29296\n",
      "huggingface     4262\n",
      "github          1664\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load all data sources\n",
    "kaggle_df = load_kaggle_data()\n",
    "huggingface_df = load_huggingface_data()\n",
    "github_df = load_github_data()\n",
    "\n",
    "# Combine all sources\n",
    "combined_df = pd.concat([kaggle_df, huggingface_df, github_df], ignore_index=True)\n",
    "print(f\"\\nTotal quotes before processing: {len(combined_df)}\")\n",
    "print(f\"Source distribution:\")\n",
    "print(combined_df['source_dataset'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 2: Data Cleaning & Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_quotes(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Standardize quote text formatting\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Remove surrounding quotation marks\n",
    "    df['quote_text'] = df['quote_text'].str.strip('\"\\'‚Äú‚Äù‚Äò‚Äô')\n",
    "    \n",
    "    # Normalize whitespace\n",
    "    df['quote_text'] = df['quote_text'].str.replace(r'\\s+', ' ', regex=True)\n",
    "    df['quote_text'] = df['quote_text'].str.strip()\n",
    "    \n",
    "    # Ensure proper sentence capitalization\n",
    "    df['quote_text'] = df['quote_text'].apply(lambda x: x[0].upper() + x[1:] if len(x) > 0 else x)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def filter_by_length(df: pd.DataFrame, min_length: int = 5, max_length: int = 150) -> pd.DataFrame:\n",
    "    \"\"\"Filter quotes by character length\"\"\"\n",
    "    initial_count = len(df)\n",
    "    \n",
    "    # Filter by length\n",
    "    df_filtered = df[\n",
    "        (df['quote_text'].str.len() >= min_length) & \n",
    "        (df['quote_text'].str.len() <= max_length)\n",
    "    ].copy()\n",
    "    \n",
    "    removed_count = initial_count - len(df_filtered)\n",
    "    print(f\"Removed {removed_count} quotes due to length constraints\")\n",
    "    \n",
    "    return df_filtered\n",
    "\n",
    "def normalize_authors(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Normalize author names\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Handle missing authors\n",
    "    df['author'] = df['author'].fillna('Unknown')\n",
    "    df['author'] = df['author'].replace('', 'Unknown')\n",
    "    \n",
    "    # Clean author names\n",
    "    df['author'] = df['author'].str.strip()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying data cleaning...\n",
      "Removed 850 quotes due to length constraints\n",
      "Quotes after cleaning: 34372\n"
     ]
    }
   ],
   "source": [
    "# Apply cleaning functions\n",
    "print(\"Applying data cleaning...\")\n",
    "cleaned_df = standardize_quotes(combined_df)\n",
    "cleaned_df = filter_by_length(cleaned_df)\n",
    "cleaned_df = normalize_authors(cleaned_df)\n",
    "\n",
    "print(f\"Quotes after cleaning: {len(cleaned_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 3: Duplicate Detection & Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_quality_score(row: pd.Series) -> int:\n",
    "    \"\"\"Calculate quality score for duplicate resolution\"\"\"\n",
    "    score = 0\n",
    "    \n",
    "    # Known author > Unknown author\n",
    "    if row['author'] != 'Unknown':\n",
    "        score += 4\n",
    "    \n",
    "    # Has categories > no categories\n",
    "    if pd.notna(row['existing_categories']) and row['existing_categories'] != '':\n",
    "        score += 2\n",
    "    \n",
    "    # Has instruction prompt > no prompt\n",
    "    if pd.notna(row['instruction_prompt']) and row['instruction_prompt'] != '':\n",
    "        score += 2\n",
    "    \n",
    "    # Source priority: HuggingFace > Kaggle > GitHub\n",
    "    if row['source_dataset'] == 'huggingface':\n",
    "        score += 3\n",
    "    elif row['source_dataset'] == 'kaggle':\n",
    "        score += 2\n",
    "    else:  # github\n",
    "        score += 1\n",
    "    \n",
    "    return score\n",
    "\n",
    "def normalize_for_comparison(text: str) -> str:\n",
    "    \"\"\"Normalize text for similarity comparison\"\"\"\n",
    "    # Convert to lowercase and remove punctuation\n",
    "    normalized = re.sub(r'[^a-z0-9\\s]', '', text.lower())\n",
    "    # Remove extra whitespace\n",
    "    normalized = re.sub(r'\\s+', ' ', normalized).strip()\n",
    "    return normalized\n",
    "\n",
    "def find_near_duplicates_fast(df: pd.DataFrame, threshold: float = 0.9) -> pd.DataFrame:\n",
    "    \"\"\"Fast near-duplicate detection using optimized algorithms\"\"\"\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    print(f\"Finding near-duplicates with {threshold} similarity threshold...\")\n",
    "    \n",
    "    # Add quality scores\n",
    "    df = df.copy()\n",
    "    df['quality_score'] = df.apply(calculate_quality_score, axis=1)\n",
    "    \n",
    "    # Step 1: Remove exact duplicates efficiently\n",
    "    initial_count = len(df)\n",
    "    df = df.sort_values('quality_score', ascending=False)\n",
    "    df = df.drop_duplicates(subset=['quote_text'], keep='first')\n",
    "    exact_duplicates_removed = initial_count - len(df)\n",
    "    print(f\"Removed {exact_duplicates_removed} exact duplicates\")\n",
    "    \n",
    "    # Step 2: Fast near-duplicate detection using length-based grouping\n",
    "    df['normalized_text'] = df['quote_text'].apply(normalize_for_comparison)\n",
    "    df['text_length'] = df['normalized_text'].str.len()\n",
    "    \n",
    "    # Group by similar lengths (¬±5 characters) to reduce comparisons\n",
    "    length_groups = {}\n",
    "    for idx, row in df.iterrows():\n",
    "        length = row['text_length']\n",
    "        length_key = length // 5  # Group by 5-character buckets\n",
    "        if length_key not in length_groups:\n",
    "            length_groups[length_key] = []\n",
    "        length_groups[length_key].append(idx)\n",
    "    \n",
    "    to_remove = set()\n",
    "    total_comparisons = 0\n",
    "    \n",
    "    print(f\"Processing {len(length_groups)} length groups...\")\n",
    "    \n",
    "    for group_key, indices in length_groups.items():\n",
    "        if len(indices) < 2:\n",
    "            continue  # Skip groups with only one quote\n",
    "            \n",
    "        # Sort group by quality score (highest first)\n",
    "        group_df = df.loc[indices].sort_values('quality_score', ascending=False)\n",
    "        group_indices = list(group_df.index)\n",
    "        \n",
    "        # Compare within group using optimized approach\n",
    "        for i, idx1 in enumerate(group_indices):\n",
    "            if idx1 in to_remove:\n",
    "                continue\n",
    "                \n",
    "            row1 = df.loc[idx1]\n",
    "            \n",
    "            # Only compare with next 20 quotes maximum to limit complexity\n",
    "            max_comparisons = min(20, len(group_indices) - i - 1)\n",
    "            \n",
    "            for j in range(1, max_comparisons + 1):\n",
    "                if i + j >= len(group_indices):\n",
    "                    break\n",
    "                    \n",
    "                idx2 = group_indices[i + j]\n",
    "                if idx2 in to_remove:\n",
    "                    continue\n",
    "                    \n",
    "                row2 = df.loc[idx2]\n",
    "                \n",
    "                # Quick length check (must be within 10 characters)\n",
    "                if abs(row1['text_length'] - row2['text_length']) > 10:\n",
    "                    continue\n",
    "                \n",
    "                # Quick first-word check for early termination\n",
    "                words1 = row1['normalized_text'].split()\n",
    "                words2 = row2['normalized_text'].split()\n",
    "                if len(words1) > 0 and len(words2) > 0 and words1[0] != words2[0]:\n",
    "                    if len(words1[0]) > 3 and len(words2[0]) > 3:\n",
    "                        continue  # Skip if first significant words are different\n",
    "                \n",
    "                # Full similarity check\n",
    "                similarity = difflib.SequenceMatcher(\n",
    "                    None, row1['normalized_text'], row2['normalized_text']\n",
    "                ).ratio()\n",
    "                \n",
    "                total_comparisons += 1\n",
    "                \n",
    "                if similarity >= threshold:\n",
    "                    # Always remove the lower quality one (idx2, since sorted by quality)\n",
    "                    to_remove.add(idx2)\n",
    "    \n",
    "    # Remove near-duplicates\n",
    "    df_deduplicated = df.drop(index=list(to_remove))\n",
    "    near_duplicates_removed = len(to_remove)\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"Removed {near_duplicates_removed} near-duplicates\")\n",
    "    print(f\"Total duplicates removed: {exact_duplicates_removed + near_duplicates_removed}\")\n",
    "    print(f\"Total comparisons made: {total_comparisons:,}\")\n",
    "    print(f\"Processing time: {elapsed_time:.1f} seconds\")\n",
    "    \n",
    "    # Clean up temporary columns and reset index\n",
    "    df_deduplicated = df_deduplicated.drop(columns=['quality_score', 'normalized_text', 'text_length'])\n",
    "    df_deduplicated = df_deduplicated.reset_index(drop=True)\n",
    "    \n",
    "    return df_deduplicated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding near-duplicates with 0.9 similarity threshold...\n",
      "Removed 552 exact duplicates\n",
      "Processing 31 length groups...\n",
      "Removed 123 near-duplicates\n",
      "Total duplicates removed: 675\n",
      "Total comparisons made: 482,482\n",
      "Processing time: 59.2 seconds\n",
      "Quotes after deduplication: 33697\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicates using fast optimized approach\n",
    "deduplicated_df = find_near_duplicates_fast(cleaned_df)\n",
    "print(f\"Quotes after deduplication: {len(deduplicated_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 4: Quality Assurance & Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final dataset size: 33697\n"
     ]
    }
   ],
   "source": [
    "# No instruction prompt generation per v2 spec - use deduplicated data directly\n",
    "final_df = deduplicated_df.copy()\n",
    "print(f\"Final dataset size: {len(final_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 5: Quality Assurance & Export"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quality Assurance & Export Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_dataset(df: pd.DataFrame) -> bool:\n",
    "    \"\"\"Validate final dataset quality\"\"\"\n",
    "    print(\"\\n=== Dataset Validation ===\")\n",
    "    \n",
    "    # Check required columns\n",
    "    required_columns = ['quote_text', 'author', 'source_dataset', 'instruction_prompt', 'existing_categories']\n",
    "    missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "    if missing_columns:\n",
    "        print(f\"‚ùå Missing columns: {missing_columns}\")\n",
    "        return False\n",
    "    print(\"‚úÖ All required columns present\")\n",
    "    \n",
    "    # Check for empty quote_text\n",
    "    empty_quotes = df['quote_text'].isna().sum() + (df['quote_text'] == '').sum()\n",
    "    if empty_quotes > 0:\n",
    "        print(f\"‚ùå {empty_quotes} empty quotes found\")\n",
    "        return False\n",
    "    print(\"‚úÖ No empty quotes\")\n",
    "    \n",
    "    # Check length constraints\n",
    "    quote_lengths = df['quote_text'].str.len()\n",
    "    invalid_lengths = ((quote_lengths < 5) | (quote_lengths > 150)).sum()\n",
    "    if invalid_lengths > 0:\n",
    "        print(f\"‚ùå {invalid_lengths} quotes with invalid length\")\n",
    "        return False\n",
    "    print(\"‚úÖ All quotes within length constraints (5-150 characters)\")\n",
    "    \n",
    "    # Check instruction prompts (OK to be empty per v2 spec)\n",
    "    empty_prompts = df['instruction_prompt'].isna().sum() + (df['instruction_prompt'] == '').sum()\n",
    "    print(f\"‚ÑπÔ∏è  {empty_prompts} entries without instruction prompts (acceptable per v2 spec)\")\n",
    "    \n",
    "    print(\"\\n‚úÖ Dataset validation passed!\")\n",
    "    return True\n",
    "\n",
    "def generate_quality_metrics(df: pd.DataFrame):\n",
    "    \"\"\"Generate and display quality metrics\"\"\"\n",
    "    print(\"\\n=== Quality Metrics ===\")\n",
    "    \n",
    "    # Source distribution\n",
    "    print(\"\\nSource Distribution:\")\n",
    "    source_dist = df['source_dataset'].value_counts()\n",
    "    for source, count in source_dist.items():\n",
    "        percentage = (count / len(df)) * 100\n",
    "        print(f\"  {source}: {count:,} ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Quote length distribution\n",
    "    print(\"\\nQuote Length Statistics:\")\n",
    "    lengths = df['quote_text'].str.len()\n",
    "    print(f\"  Mean: {lengths.mean():.1f} characters\")\n",
    "    print(f\"  Median: {lengths.median():.1f} characters\")\n",
    "    print(f\"  Min: {lengths.min()} characters\")\n",
    "    print(f\"  Max: {lengths.max()} characters\")\n",
    "    \n",
    "    # Author coverage\n",
    "    print(\"\\nAuthor Statistics:\")\n",
    "    unique_authors = df['author'].nunique()\n",
    "    unknown_authors = (df['author'] == 'Unknown').sum()\n",
    "    print(f\"  Unique authors: {unique_authors:,}\")\n",
    "    print(f\"  Unknown authors: {unknown_authors:,} ({(unknown_authors/len(df)*100):.1f}%)\")\n",
    "    \n",
    "    # Top authors\n",
    "    print(\"\\nTop 10 Authors:\")\n",
    "    top_authors = df['author'].value_counts().head(10)\n",
    "    for author, count in top_authors.items():\n",
    "        print(f\"  {author}: {count} quotes\")\n",
    "    \n",
    "    # Instruction prompt analysis\n",
    "    print(\"\\nInstruction Prompt Analysis:\")\n",
    "    with_prompts = df[df['instruction_prompt'] != '']\n",
    "    if len(with_prompts) > 0:\n",
    "        prompt_dist = with_prompts['instruction_prompt'].value_counts().head(5)\n",
    "        for prompt, count in prompt_dist.items():\n",
    "            print(f\"  '{prompt}': {count} quotes\")\n",
    "    else:\n",
    "        print(\"  No instruction prompts present\")\n",
    "    \n",
    "    # Categories analysis\n",
    "    print(\"\\nCategory Analysis:\")\n",
    "    with_categories = df[df['existing_categories'] != '']\n",
    "    if len(with_categories) > 0:\n",
    "        print(f\"  Entries with categories: {len(with_categories):,}\")\n",
    "        category_sample = with_categories['existing_categories'].value_counts().head(5)\n",
    "        for category, count in category_sample.items():\n",
    "            print(f\"  '{category}': {count} quotes\")\n",
    "    else:\n",
    "        print(\"  No categories present\")\n",
    "    \n",
    "    return {\n",
    "        'total_records': len(df),\n",
    "        'source_distribution': source_dist.to_dict(),\n",
    "        'length_stats': {\n",
    "            'mean': lengths.mean(),\n",
    "            'median': lengths.median(),\n",
    "            'min': lengths.min(),\n",
    "            'max': lengths.max()\n",
    "        },\n",
    "        'unique_authors': unique_authors,\n",
    "        'unknown_authors': unknown_authors\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Validate dataset using proper function\nis_valid = validate_dataset(final_df)\n\nif is_valid:\n    # Generate quality metrics using proper function\n    metrics = generate_quality_metrics(final_df)\n    \n    # Export to CSV\n    final_df.to_csv(OUTPUT_PATH, index=False)\n    print(f\"\\n‚úÖ Dataset exported to: {OUTPUT_PATH}\")\n    print(f\"üìä Final dataset contains {len(final_df):,} high-quality quotes\")\n    \n    # Save summary report\n    summary_path = \"../data/processed/processing_summary.txt\"\n    with open(summary_path, \"w\") as f:\n        f.write(\"Quote Processing Summary\\n\")\n        f.write(\"======================\\n\\n\")\n        f.write(f\"Total quotes processed: {len(final_df):,}\\n\")\n        f.write(f\"Source distribution:\\n\")\n        for source, count in metrics['source_distribution'].items():\n            f.write(f\"  {source}: {count:,}\\n\")\n        f.write(f\"\\nUnique authors: {metrics['unique_authors']:,}\\n\")\n        f.write(f\"Unknown authors: {metrics['unknown_authors']:,}\\n\")\n        f.write(f\"\\nQuote length range: {metrics['length_stats']['min']}-{metrics['length_stats']['max']} characters\\n\")\n        f.write(f\"Average length: {metrics['length_stats']['mean']:.1f} characters\\n\")\n    \n    print(f\"üìÑ Summary report saved to: {summary_path}\")\n    \nelse:\n    print(\"‚ùå Dataset validation failed. Please fix issues before export.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Data Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Sample Quotes ===\n",
      "\n",
      "Quote: \"By changing how you perceive things and how you act upon those perceptions, you will change your life.\"\n",
      "Author: Chris Prentiss, Be Who You Want, Have What You Want: Change Your Thinking, Change Your Life\n",
      "Source: kaggle\n",
      "Categories: change, chris-prentiss, inspiration, inspirational, life, non-12-step, passages-malibu, passages-ventura, perception, philosophy, quotes\n",
      "--------------------------------------------------\n",
      "\n",
      "Quote: \"There's no time for hatred, only questions. Where is love? Where is happiness? What is life? Where is peace?\"\n",
      "Author: Jeff Buckley\n",
      "Source: kaggle\n",
      "Categories: advice, inspirational, jeff-buckley, life, love, no-hatred, peace\n",
      "--------------------------------------------------\n",
      "\n",
      "Quote: \"A purpose directed disciplined action always bring success.\"\n",
      "Author: Debasish Mridha\n",
      "Source: kaggle\n",
      "Categories: action, directed, disciplined, education, happiness, hope, inspirational, intelligence, knowledge, life, love, philosophy, purpose, success, truth, wisdom\n",
      "--------------------------------------------------\n",
      "\n",
      "Quote: \"My children taught me the true meaning of unconditional love.\"\n",
      "Author: Yvonne Pierre, The Day My Soul Cried:  A Memoir\n",
      "Source: kaggle\n",
      "Categories: children, down-syndrome, inspirational, parent, parenting, special-needs\n",
      "--------------------------------------------------\n",
      "\n",
      "Quote: \"Change comes by substituting good habits for less desirable ones. You mold your character and future by good thoughts and acts.\"\n",
      "Author: Spencer W. Kimball\n",
      "Source: kaggle\n",
      "Categories: inspirational-life\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Display sample data\n",
    "print(\"\\n=== Sample Quotes ===\")\n",
    "sample_quotes = final_df.sample(5, random_state=42)\n",
    "for idx, row in sample_quotes.iterrows():\n",
    "    print(f\"\\nQuote: \\\"{row['quote_text']}\\\"\")\n",
    "    print(f\"Author: {row['author']}\")\n",
    "    print(f\"Source: {row['source_dataset']}\")\n",
    "    if row['instruction_prompt']:\n",
    "        print(f\"Prompt: {row['instruction_prompt']}\")\n",
    "    if row['existing_categories']:\n",
    "        print(f\"Categories: {row['existing_categories']}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quote-finetuning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}