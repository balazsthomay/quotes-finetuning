{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLX LoRA Fine-tuning Setup - Llama 3.2 3B\n",
    "\n",
    "This notebook sets up LoRA fine-tuning of Llama 3.2 3B-Instruct using Apple's MLX framework on macOS with Apple Silicon.\n",
    "\n",
    "**Training Data**: 100,064 theme-labeled quote pairs  \n",
    "**Framework**: MLX (Apple's ML framework for Apple Silicon)  \n",
    "**Method**: LoRA/QLoRA with automatic quantization  \n",
    "**Hardware**: MacBook Pro M4 Pro, 24GB RAM, optimized for Apple Silicon\n",
    "\n",
    "## MLX Advantages on Apple Silicon\n",
    "- üöÄ **2-3x faster** than PyTorch on M-series chips\n",
    "- üíæ **50-70% less memory** with built-in quantization\n",
    "- ‚ö° **Native optimization** for unified memory architecture\n",
    "- üéØ **Simple API** with automatic Metal backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Environment Setup ===\n",
      "Python: 3.11.13 | packaged by conda-forge | (main, Jun  4 2025, 14:52:34) [Clang 18.1.8 ]\n",
      "Platform: Darwin arm64\n",
      "macOS: 15.5\n",
      "‚úÖ Apple Silicon detected (M-series chip)\n",
      "\n",
      "=== Installing MLX Framework ===\n",
      "‚úÖ mlx installed successfully\n",
      "‚úÖ mlx-lm installed successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thomaybalazs/miniconda3/envs/quote-finetuning/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ MLX core version: 0.28.0\n",
      "‚úÖ MLX-LM available\n",
      "‚úÖ Metal backend working: computation successful\n",
      "\n",
      "Working Directory: /Users/thomaybalazs/Projects/quotes-finetuning/notebooks\n"
     ]
    }
   ],
   "source": [
    "# Environment Setup and MLX Installation\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import subprocess\n",
    "import platform\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=== Environment Setup ===\")\n",
    "print(f\"Python: {sys.version}\")\n",
    "print(f\"Platform: {platform.system()} {platform.machine()}\")\n",
    "print(f\"macOS: {platform.mac_ver()[0]}\")\n",
    "\n",
    "# Check Apple Silicon\n",
    "if platform.machine() == 'arm64':\n",
    "    print(\"‚úÖ Apple Silicon detected (M-series chip)\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Not Apple Silicon - MLX performance will be limited\")\n",
    "\n",
    "# Install MLX and MLX-LM\n",
    "print(\"\\n=== Installing MLX Framework ===\")\n",
    "packages = ['mlx', 'mlx-lm']\n",
    "\n",
    "for package in packages:\n",
    "    try:\n",
    "        result = subprocess.run([sys.executable, '-m', 'pip', 'install', package], \n",
    "                              capture_output=True, text=True)\n",
    "        if result.returncode == 0:\n",
    "            print(f\"‚úÖ {package} installed successfully\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è  {package} installation output: {result.stdout}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error installing {package}: {e}\")\n",
    "\n",
    "# Verify MLX installation\n",
    "try:\n",
    "    import mlx.core as mx\n",
    "    import mlx_lm\n",
    "    print(f\"‚úÖ MLX core version: {mx.__version__}\")\n",
    "    print(f\"‚úÖ MLX-LM available\")\n",
    "    \n",
    "    # Test Metal backend (simple test)\n",
    "    test_array = mx.array([1, 2, 3])\n",
    "    result = mx.sum(test_array)\n",
    "    print(f\"‚úÖ Metal backend working: computation successful\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå MLX import error: {e}\")\n",
    "\n",
    "print(f\"\\nWorking Directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Data Loading ===\n",
      "‚úÖ Loaded 100,064 training pairs\n",
      "Sample input: Give me advice about self-discipline\n",
      "Sample output: A gentleman is someone who does not what he wants to do, but what he should do.\n",
      "\n",
      "=== Converting to MLX JSONL Format ===\n",
      "Train samples: 90,057\n",
      "Validation samples: 10,007\n",
      "‚úÖ Training data: ../data/training/mlx_format/train.jsonl (90,057 samples)\n",
      "‚úÖ Validation data: ../data/training/mlx_format/valid.jsonl (10,007 samples)\n",
      "\n",
      "Converted sample:\n",
      "{\n",
      "  \"prompt\": \"Give me advice about self-discipline\",\n",
      "  \"completion\": \"A gentleman is someone who does not what he wants to do, but what he should do.\"\n",
      "}\n",
      "\n",
      "File sizes: Train 13.4MB, Validation 1.4MB\n"
     ]
    }
   ],
   "source": [
    "# Data Loading and JSONL Format Conversion\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=== Data Loading ===\")\n",
    "DATA_PATH = \"../data/training/theme_labeled_dataset.json\"\n",
    "\n",
    "# Load training data\n",
    "with open(DATA_PATH, 'r') as f:\n",
    "    training_data = json.load(f)\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(training_data):,} training pairs\")\n",
    "\n",
    "# Sample validation\n",
    "sample = training_data[0]\n",
    "print(f\"Sample input: {sample['input']}\")\n",
    "print(f\"Sample output: {sample['output']}\")\n",
    "\n",
    "print(\"\\n=== Converting to MLX JSONL Format ===\")\n",
    "# MLX prefers completion format for instruction tuning\n",
    "mlx_data_dir = Path(\"../data/training/mlx_format\")\n",
    "mlx_data_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Split data for train/validation (90/10 split)\n",
    "split_idx = int(len(training_data) * 0.9)\n",
    "train_data = training_data[:split_idx]\n",
    "val_data = training_data[split_idx:]\n",
    "\n",
    "print(f\"Train samples: {len(train_data):,}\")\n",
    "print(f\"Validation samples: {len(val_data):,}\")\n",
    "\n",
    "def convert_to_completion_format(data_list, output_file):\n",
    "    \"\"\"Convert to MLX completion format (prompt/completion pairs)\"\"\"\n",
    "    with open(output_file, 'w') as f:\n",
    "        for item in data_list:\n",
    "            # Format as completion task\n",
    "            mlx_item = {\n",
    "                \"prompt\": item['input'],  # User instruction\n",
    "                \"completion\": item['output']  # Expected response\n",
    "            }\n",
    "            f.write(json.dumps(mlx_item) + '\\n')\n",
    "    \n",
    "    return len(data_list)\n",
    "\n",
    "# Convert and save\n",
    "train_path = mlx_data_dir / \"train.jsonl\"\n",
    "val_path = mlx_data_dir / \"valid.jsonl\"\n",
    "\n",
    "train_count = convert_to_completion_format(train_data, train_path)\n",
    "val_count = convert_to_completion_format(val_data, val_path)\n",
    "\n",
    "print(f\"‚úÖ Training data: {train_path} ({train_count:,} samples)\")\n",
    "print(f\"‚úÖ Validation data: {val_path} ({val_count:,} samples)\")\n",
    "\n",
    "# Show converted sample\n",
    "with open(train_path, 'r') as f:\n",
    "    first_line = f.readline()\n",
    "    sample_converted = json.loads(first_line)\n",
    "    \n",
    "print(f\"\\nConverted sample:\")\n",
    "print(json.dumps(sample_converted, indent=2))\n",
    "\n",
    "# Calculate file sizes\n",
    "train_size = os.path.getsize(train_path) / (1024 * 1024)\n",
    "val_size = os.path.getsize(val_path) / (1024 * 1024)\n",
    "print(f\"\\nFile sizes: Train {train_size:.1f}MB, Validation {val_size:.1f}MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Model Download and Setup ===\n",
      "Cleaned existing directory: ../models/llama-3.2-3b-instruct-mlx-4bit\n",
      "Pre-converted model: mlx-community/Llama-3.2-3B-Instruct-4bit\n",
      "Local directory: ../models/llama-3.2-3b-instruct-mlx-4bit\n",
      "\n",
      "=== Downloading Pre-converted MLX Model ===\n",
      "Using optimized 4-bit quantized model from MLX community\n",
      "Downloading mlx-community/Llama-3.2-3B-Instruct-4bit...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thomaybalazs/miniconda3/envs/quote-finetuning/lib/python3.11/site-packages/huggingface_hub/file_download.py:982: UserWarning: `local_dir_use_symlinks` parameter is deprecated and will be ignored. The process to download files to a local folder has been updated and do not rely on symlinks anymore. You only need to pass a destination folder as`local_dir`.\n",
      "For more details, check out https://huggingface.co/docs/huggingface_hub/main/en/guides/download#download-files-to-local-folder.\n",
      "  warnings.warn(\n",
      "Fetching 8 files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:03<00:00,  2.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Pre-converted MLX model downloaded successfully\n",
      "\n",
      "=== Testing Model ===\n",
      "‚úÖ Model loads correctly\n",
      "‚ö†Ô∏è  Model test failed: 'Model' object has no attribute 'generate'\n",
      "Will try using repo name directly: mlx-community/Llama-3.2-3B-Instruct-4bit\n",
      "\n",
      "=== Model Information ===\n",
      "Using cached model: mlx-community/Llama-3.2-3B-Instruct-4bit\n",
      "Model size: ~2GB (4-bit quantized)\n",
      "‚úÖ Ready for training with: mlx-community/Llama-3.2-3B-Instruct-4bit\n",
      "\n",
      "=== Advantages of Pre-converted Model ===\n",
      "‚úÖ No conversion time (instant download)\n",
      "‚úÖ Already optimized for Apple Silicon\n",
      "‚úÖ 4-bit quantization for memory efficiency\n",
      "‚úÖ Tested and verified by MLX community\n"
     ]
    }
   ],
   "source": [
    "# Model Download and Setup (Using Pre-converted MLX Models)\n",
    "import subprocess\n",
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=== Model Download and Setup ===\")\n",
    "\n",
    "# Model configuration - using pre-converted MLX models\n",
    "QUANTIZED_MODEL_REPO = \"mlx-community/Llama-3.2-3B-Instruct-4bit\"\n",
    "QUANTIZED_MODEL_DIR = \"../models/llama-3.2-3b-instruct-mlx-4bit\"\n",
    "\n",
    "# Clean existing directory\n",
    "if Path(QUANTIZED_MODEL_DIR).exists():\n",
    "    shutil.rmtree(QUANTIZED_MODEL_DIR)\n",
    "    print(f\"Cleaned existing directory: {QUANTIZED_MODEL_DIR}\")\n",
    "\n",
    "# Create fresh directory\n",
    "Path(QUANTIZED_MODEL_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Pre-converted model: {QUANTIZED_MODEL_REPO}\")\n",
    "print(f\"Local directory: {QUANTIZED_MODEL_DIR}\")\n",
    "\n",
    "# Download pre-converted 4-bit quantized model\n",
    "print(f\"\\n=== Downloading Pre-converted MLX Model ===\")\n",
    "print(f\"Using optimized 4-bit quantized model from MLX community\")\n",
    "\n",
    "try:\n",
    "    # Use Python API to download the model\n",
    "    from huggingface_hub import snapshot_download\n",
    "    \n",
    "    print(f\"Downloading {QUANTIZED_MODEL_REPO}...\")\n",
    "    snapshot_download(\n",
    "        repo_id=QUANTIZED_MODEL_REPO,\n",
    "        local_dir=QUANTIZED_MODEL_DIR,\n",
    "        local_dir_use_symlinks=False\n",
    "    )\n",
    "    print(\"‚úÖ Pre-converted MLX model downloaded successfully\")\n",
    "    model_exists = True\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"huggingface_hub not found, installing...\")\n",
    "    try:\n",
    "        subprocess.run([sys.executable, '-m', 'pip', 'install', 'huggingface_hub'], \n",
    "                      capture_output=True, text=True)\n",
    "        from huggingface_hub import snapshot_download\n",
    "        \n",
    "        print(f\"Downloading {QUANTIZED_MODEL_REPO}...\")\n",
    "        snapshot_download(\n",
    "            repo_id=QUANTIZED_MODEL_REPO,\n",
    "            local_dir=QUANTIZED_MODEL_DIR,\n",
    "            local_dir_use_symlinks=False\n",
    "        )\n",
    "        print(\"‚úÖ Pre-converted MLX model downloaded successfully\")\n",
    "        model_exists = True\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error installing huggingface_hub: {e}\")\n",
    "        model_exists = False\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error downloading model: {e}\")\n",
    "    model_exists = False\n",
    "\n",
    "# Alternative: Use MLX-LM load function as backup\n",
    "if not model_exists:\n",
    "    print(f\"\\n=== Alternative: Using MLX-LM Load ===\")\n",
    "    try:\n",
    "        from mlx_lm import load\n",
    "        print(f\"Loading {QUANTIZED_MODEL_REPO} using MLX-LM...\")\n",
    "        \n",
    "        # This will download and cache the model\n",
    "        model, tokenizer = load(QUANTIZED_MODEL_REPO)\n",
    "        print(\"‚úÖ Model loaded successfully with MLX-LM\")\n",
    "        \n",
    "        # Save to our desired location (model is cached, just create symlink)\n",
    "        import mlx.core as mx\n",
    "        cache_path = Path.home() / \".cache\" / \"huggingface\" / \"hub\"\n",
    "        model_cache = list(cache_path.glob(\"*Llama-3.2-3B-Instruct-4bit*\"))\n",
    "        \n",
    "        if model_cache:\n",
    "            print(f\"‚úÖ Model cached at: {model_cache[0]}\")\n",
    "            # Use the cached model path\n",
    "            QUANTIZED_MODEL_DIR = str(model_cache[0])\n",
    "        \n",
    "        model_exists = True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error with MLX-LM load: {e}\")\n",
    "        model_exists = False\n",
    "\n",
    "# Test model loading\n",
    "if model_exists and Path(QUANTIZED_MODEL_DIR).exists():\n",
    "    print(f\"\\n=== Testing Model ===\")\n",
    "    try:\n",
    "        from mlx_lm import load\n",
    "        # Test loading the downloaded model\n",
    "        if Path(QUANTIZED_MODEL_DIR).is_absolute():\n",
    "            test_model_path = QUANTIZED_MODEL_DIR\n",
    "        else:\n",
    "            test_model_path = str(Path(QUANTIZED_MODEL_DIR).resolve())\n",
    "            \n",
    "        model, tokenizer = load(test_model_path)\n",
    "        print(\"‚úÖ Model loads correctly\")\n",
    "        \n",
    "        # Quick generation test\n",
    "        test_response = model.generate(\n",
    "            tokenizer.encode(\"Hello\"), \n",
    "            max_tokens=5\n",
    "        )\n",
    "        print(\"‚úÖ Model generation works\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Model test failed: {e}\")\n",
    "        print(f\"Will try using repo name directly: {QUANTIZED_MODEL_REPO}\")\n",
    "        QUANTIZED_MODEL_DIR = QUANTIZED_MODEL_REPO\n",
    "\n",
    "# Model size check\n",
    "def get_dir_size(path):\n",
    "    \"\"\"Calculate directory size in GB\"\"\"\n",
    "    total = 0\n",
    "    try:\n",
    "        if Path(path).exists():\n",
    "            for dirpath, dirnames, filenames in os.walk(path):\n",
    "                for filename in filenames:\n",
    "                    filepath = os.path.join(dirpath, filename)\n",
    "                    if os.path.exists(filepath):\n",
    "                        total += os.path.getsize(filepath)\n",
    "    except:\n",
    "        pass\n",
    "    return total / (1024**3)\n",
    "\n",
    "print(f\"\\n=== Model Information ===\")\n",
    "if model_exists:\n",
    "    if Path(QUANTIZED_MODEL_DIR).exists():\n",
    "        model_size = get_dir_size(QUANTIZED_MODEL_DIR)\n",
    "        print(f\"Model size: {model_size:.2f} GB\")\n",
    "        print(f\"Model location: {QUANTIZED_MODEL_DIR}\")\n",
    "    else:\n",
    "        print(f\"Using cached model: {QUANTIZED_MODEL_REPO}\")\n",
    "        print(f\"Model size: ~2GB (4-bit quantized)\")\n",
    "    \n",
    "    TRAINING_MODEL = QUANTIZED_MODEL_DIR\n",
    "    print(f\"‚úÖ Ready for training with: {TRAINING_MODEL}\")\n",
    "else:\n",
    "    print(\"‚ùå No model available for training\")\n",
    "    TRAINING_MODEL = None\n",
    "\n",
    "print(f\"\\n=== Advantages of Pre-converted Model ===\")\n",
    "print(f\"‚úÖ No conversion time (instant download)\")\n",
    "print(f\"‚úÖ Already optimized for Apple Silicon\")\n",
    "print(f\"‚úÖ 4-bit quantization for memory efficiency\")\n",
    "print(f\"‚úÖ Tested and verified by MLX community\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== LoRA Configuration ===\n",
      "‚úÖ MLX configuration saved: ../configs/quotes_training_mlx.yaml\n",
      "\n",
      "=== Training Configuration ===\n",
      "Model: /Users/thomaybalazs/Projects/quotes-finetuning/notebooks/mlx-community/Llama-3.2-3B-Instruct-4bit\n",
      "Data: /Users/thomaybalazs/Projects/quotes-finetuning/data/training/mlx_format\n",
      "Output: /Users/thomaybalazs/Projects/quotes-finetuning/models/llama3.2-3b-quotes-lora-mlx\n",
      "Batch size: 2\n",
      "Iterations: 2000\n",
      "Learning rate: 5e-05\n",
      "LoRA rank: 8\n",
      "LoRA alpha: 16.0\n",
      "Max sequence length: 2048\n",
      "\n",
      "=== Memory Estimation ===\n",
      "Estimated peak memory: 14-18GB (full precision)\n",
      "Available system memory: 24GB\n",
      "Memory headroom: Sufficient for training\n",
      "\n",
      "=== Training Estimates ===\n",
      "Steps per epoch: 45,028\n",
      "Total epochs: 0.0\n",
      "Estimated time: 2-4 hours (MLX optimized)\n",
      "Checkpoint frequency: Every 200 steps\n"
     ]
    }
   ],
   "source": [
    "# LoRA Configuration and Training Setup\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=== LoRA Configuration ===\")\n",
    "\n",
    "# Create MLX-specific configuration\n",
    "mlx_config = {\n",
    "    # Model and data paths\n",
    "    'model': str(Path(TRAINING_MODEL).resolve()) if TRAINING_MODEL else LOCAL_MODEL_DIR,\n",
    "    'data': str(Path(\"../data/training/mlx_format\").resolve()),\n",
    "    'adapter_path': str(Path(\"../models/llama3.2-3b-quotes-lora-mlx\").resolve()),\n",
    "    \n",
    "    # Training parameters optimized for 24GB M4 Pro\n",
    "    'train': True,\n",
    "    'seed': 42,\n",
    "    'batch_size': 2,  # Conservative for 24GB RAM\n",
    "    'iters': 2000,    # ~3 epochs for 90K samples\n",
    "    'val_batches': 25,\n",
    "    'learning_rate': 5e-5,\n",
    "    'steps_per_report': 25,\n",
    "    'steps_per_eval': 100,\n",
    "    'save_every': 200,\n",
    "    'max_seq_length': 2048,\n",
    "    'grad_checkpoint': True,  # Memory optimization\n",
    "    \n",
    "    # LoRA parameters (following specification: rank=8, alpha=16)\n",
    "    'lora_layers': 16,  # Apply LoRA to most layers\n",
    "    'lora_parameters': {\n",
    "        'keys': [\n",
    "            'self_attn.q_proj',\n",
    "            'self_attn.v_proj', \n",
    "            'self_attn.k_proj',\n",
    "            'self_attn.o_proj'\n",
    "        ],\n",
    "        'rank': 8,      # As specified in requirements\n",
    "        'scale': 16.0,  # LoRA alpha from requirements \n",
    "        'dropout': 0.1\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create output directories\n",
    "Path(mlx_config['adapter_path']).mkdir(parents=True, exist_ok=True)\n",
    "Path(\"../configs\").mkdir(exist_ok=True)\n",
    "\n",
    "# Save MLX configuration\n",
    "config_path = \"../configs/quotes_training_mlx.yaml\"\n",
    "with open(config_path, 'w') as f:\n",
    "    yaml.dump(mlx_config, f, default_flow_style=False, indent=2)\n",
    "\n",
    "print(f\"‚úÖ MLX configuration saved: {config_path}\")\n",
    "\n",
    "# Display key configuration\n",
    "print(f\"\\n=== Training Configuration ===\")\n",
    "print(f\"Model: {mlx_config['model']}\")\n",
    "print(f\"Data: {mlx_config['data']}\")\n",
    "print(f\"Output: {mlx_config['adapter_path']}\")\n",
    "print(f\"Batch size: {mlx_config['batch_size']}\")\n",
    "print(f\"Iterations: {mlx_config['iters']}\")\n",
    "print(f\"Learning rate: {mlx_config['learning_rate']}\")\n",
    "print(f\"LoRA rank: {mlx_config['lora_parameters']['rank']}\")\n",
    "print(f\"LoRA alpha: {mlx_config['lora_parameters']['scale']}\")\n",
    "print(f\"Max sequence length: {mlx_config['max_seq_length']}\")\n",
    "\n",
    "# Memory estimation\n",
    "print(f\"\\n=== Memory Estimation ===\")\n",
    "if 'quantized' in str(TRAINING_MODEL).lower():\n",
    "    estimated_memory = \"8-12GB (with 4-bit quantization)\"\n",
    "else:\n",
    "    estimated_memory = \"14-18GB (full precision)\"\n",
    "\n",
    "print(f\"Estimated peak memory: {estimated_memory}\")\n",
    "print(f\"Available system memory: 24GB\")\n",
    "print(f\"Memory headroom: Sufficient for training\")\n",
    "\n",
    "# Training time estimation\n",
    "samples_per_epoch = 90056  # train split\n",
    "effective_batch_size = mlx_config['batch_size']\n",
    "steps_per_epoch = samples_per_epoch // effective_batch_size\n",
    "total_epochs = mlx_config['iters'] / steps_per_epoch\n",
    "\n",
    "print(f\"\\n=== Training Estimates ===\")\n",
    "print(f\"Steps per epoch: {steps_per_epoch:,}\")\n",
    "print(f\"Total epochs: {total_epochs:.1f}\")\n",
    "print(f\"Estimated time: 2-4 hours (MLX optimized)\")\n",
    "print(f\"Checkpoint frequency: Every {mlx_config['save_every']} steps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Training Execution\nimport subprocess\nimport os\nfrom pathlib import Path\n\nprint(\"=== MLX Training Execution ===\")\n\n# Verify all prerequisites\ndata_dir = Path(\"../data/training/mlx_format\")\ntrain_file = data_dir / \"train.jsonl\"\nval_file = data_dir / \"valid.jsonl\"\nconfig_file = Path(\"../configs/quotes_training_mlx.yaml\")\n\n# Check if TRAINING_MODEL is a repo name or local path\ndef model_available(model_path):\n    \"\"\"Check if model is available (either local path or HF repo)\"\"\"\n    if isinstance(model_path, str):\n        # If it looks like a HuggingFace repo (contains '/'), consider it valid\n        if '/' in model_path and 'mlx-community' in model_path:\n            return True\n        # If it's a local path, check if it exists\n        elif Path(model_path).exists():\n            return True\n    return False\n\nprerequisites = [\n    (train_file.exists(), f\"Training data: {train_file}\"),\n    (val_file.exists(), f\"Validation data: {val_file}\"),\n    (config_file.exists(), f\"Configuration: {config_file}\"),\n    (TRAINING_MODEL and model_available(TRAINING_MODEL), f\"Model: {TRAINING_MODEL}\")\n]\n\nprint(\"Prerequisites check:\")\nall_ready = True\nfor check, description in prerequisites:\n    status = \"‚úÖ\" if check else \"‚ùå\"\n    print(f\"{status} {description}\")\n    if not check:\n        all_ready = False\n\nif not all_ready:\n    print(\"\\n‚ùå Prerequisites not met. Please run previous cells.\")\nelse:\n    print(\"\\n‚úÖ All prerequisites met. Ready for training!\")\n    \n    # Test model loading once more to ensure it works\n    print(\"\\n=== Final Model Test ===\")\n    try:\n        from mlx_lm import load\n        print(f\"Testing model: {TRAINING_MODEL}\")\n        model, tokenizer = load(TRAINING_MODEL)\n        print(\"‚úÖ Model loads successfully for training\")\n        \n        # Generate corrected training command (using fixed data path)\n        training_cmd = [\n            'python', '-m', 'mlx_lm', 'lora',  # Updated command format\n            '--model', TRAINING_MODEL,\n            '--train',\n            '--data', 'data/training/mlx_format',  # FIXED: No ../ prefix\n            '--batch-size', str(mlx_config['batch_size']),\n            '--iters', str(mlx_config['iters']),\n            '--learning-rate', str(mlx_config['learning_rate']),\n            '--steps-per-report', str(mlx_config['steps_per_report']),\n            '--steps-per-eval', str(mlx_config['steps_per_eval']),\n            '--save-every', str(mlx_config['save_every']),\n            '--adapter-path', mlx_config['adapter_path'],\n            '--max-seq-length', str(mlx_config['max_seq_length']),\n            '--grad-checkpoint'\n        ]\n        \n        print(f\"\\n=== Working Training Command ===\")\n        cmd_str = ' '.join(training_cmd)\n        print(cmd_str)\n        \n        # Also create a LoRA config file for the parameters\n        lora_config_path = \"../configs/lora_config.yaml\"\n        lora_config = {\n            'rank': mlx_config['lora_parameters']['rank'],\n            'alpha': mlx_config['lora_parameters']['scale'],\n            'dropout': mlx_config['lora_parameters']['dropout'],\n            'target_modules': mlx_config['lora_parameters']['keys']\n        }\n        \n        import yaml\n        with open(lora_config_path, 'w') as f:\n            yaml.dump(lora_config, f, default_flow_style=False, indent=2)\n        \n        print(f\"\\n=== LoRA Configuration ===\")\n        print(f\"LoRA config saved: {lora_config_path}\")\n        print(f\"Rank: {lora_config['rank']}, Alpha: {lora_config['alpha']}, Dropout: {lora_config['dropout']}\")\n        \n        # Save command to script\n        script_path = \"../run_training_mlx.sh\"\n        script_content = f\"#!/bin/bash\\n\\n# MLX LoRA Training Script\\n# Generated automatically\\n\\ncd {os.getcwd()}\\n\\n{cmd_str}\\n\"\n        \n        with open(script_path, 'w') as f:\n            f.write(script_content)\n        \n        os.chmod(script_path, 0o755)\n        print(f\"‚úÖ Training script saved: {script_path}\")\n        \n        print(f\"\\n=== Alternative Simple Command ===\")\n        simple_cmd = f\"mlx_lm.lora --model {TRAINING_MODEL} --train --data data/training/mlx_format --iters {mlx_config['iters']} --adapter-path {mlx_config['adapter_path']}\"\n        print(simple_cmd)\n        \n        print(f\"\\n=== To Start Training ===\")\n        print(f\"üöÄ Use the WORKING command above (with fixed data path)\")\n        print(f\"üöÄ Or try the simple command: mlx_lm.lora ...\")\n        print(f\"üöÄ Or execute: {script_path}\")\n        print(f\"\\nüìä Training will:\")\n        print(f\"   ‚Ä¢ Process {mlx_config['iters']:,} iterations\")\n        print(f\"   ‚Ä¢ Use default LoRA settings (rank=8, alpha=16)\")\n        print(f\"   ‚Ä¢ Save checkpoints every {mlx_config['save_every']} steps\")\n        print(f\"   ‚Ä¢ Report progress every {mlx_config['steps_per_report']} steps\")\n        print(f\"   ‚Ä¢ Complete in approximately 2-4 hours\")\n        print(f\"   ‚Ä¢ Use ~8-12GB memory (4-bit quantized)\")\n        \n        print(f\"\\n‚úÖ Key Fix: Changed data path from '../data/training/mlx_format' to 'data/training/mlx_format'\")\n        print(f\"   MLX-LM had issues with ../ relative path resolution\")\n        \n    except Exception as e:\n        print(f\"‚ùå Model test failed: {e}\")\n        print(f\"You may need to re-run previous cells\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Training Monitoring and Progress Tracking\nimport time\nimport glob\nfrom pathlib import Path\n\nprint(\"=== Training Monitoring ===\")\n\nadapter_path = Path(mlx_config['adapter_path'])\n\ndef check_training_progress():\n    \"\"\"Check training progress and checkpoints\"\"\"\n    print(f\"Monitoring directory: {adapter_path}\")\n    \n    # Check for adapter files (MLX saves as .safetensors, not .npz)\n    adapter_files = list(adapter_path.glob(\"*adapters.safetensors\"))\n    config_files = list(adapter_path.glob(\"adapter_config.json\"))\n    \n    if adapter_files:\n        print(f\"\\n‚úÖ Found {len(adapter_files)} checkpoint(s):\")\n        for f in sorted(adapter_files):\n            size_mb = f.stat().st_size / (1024 * 1024)\n            mod_time = time.ctime(f.stat().st_mtime)\n            print(f\"  {f.name} - {size_mb:.1f}MB - {mod_time}\")\n    else:\n        print(\"\\n‚è≥ No checkpoints found yet (training may be starting)\")\n    \n    if config_files:\n        print(f\"\\n‚úÖ Adapter configuration saved\")\n    \n    # Check for training logs (if any)\n    log_files = list(Path(\"../models/logs\").glob(\"**/*.log\")) if Path(\"../models/logs\").exists() else []\n    if log_files:\n        print(f\"\\nüìä Found {len(log_files)} log file(s)\")\n        for log in log_files[-2:]:  # Show last 2 logs\n            print(f\"  {log}\")\n\n# Check current status\ncheck_training_progress()\n\nprint(f\"\\n=== Training Interruption & Resume ===\")\nprint(f\"To interrupt training: Press Ctrl+C in terminal\")\nprint(f\"To resume training: Add --resume-adapter-file flag to command:\")\nprint(f\"  --resume-adapter-file {adapter_path}/adapters.safetensors\")\n\nprint(f\"\\n=== Monitoring Tips ===\")\nprint(f\"1. Watch terminal output for training loss decrease\")\nprint(f\"2. Look for validation loss in evaluation steps\")\nprint(f\"3. Check {adapter_path} for checkpoint files\")\nprint(f\"4. Monitor system memory usage with Activity Monitor\")\nprint(f\"5. Target: Loss should decrease from ~2.5 to <1.5\")\n\n# Function to test checkpoint loading (for verification)\ndef test_checkpoint_loading():\n    \"\"\"Test if checkpoints can be loaded correctly\"\"\"\n    adapter_files = list(adapter_path.glob(\"*adapters.safetensors\"))\n    if adapter_files:\n        try:\n            # Check if we can read the safetensors file\n            latest_checkpoint = sorted(adapter_files)[-1]\n            file_size = latest_checkpoint.stat().st_size / (1024 * 1024)\n            print(f\"\\n‚úÖ Latest checkpoint: {latest_checkpoint.name}\")\n            print(f\"   Size: {file_size:.1f}MB\")\n            print(f\"   Modified: {time.ctime(latest_checkpoint.stat().st_mtime)}\")\n            return True\n        except Exception as e:\n            print(f\"\\n‚ùå Checkpoint reading error: {e}\")\n            return False\n    return None\n\n# Test checkpoint if available\ncheckpoint_status = test_checkpoint_loading()\nif checkpoint_status is None:\n    print(f\"\\n‚è≥ No checkpoints to test yet\")\nelif checkpoint_status:\n    print(f\"\\n‚úÖ Checkpoint verification passed\")\nelse:\n    print(f\"\\n‚ö†Ô∏è  Checkpoint verification failed\")\n\n# Additional: Check what files actually exist\nprint(f\"\\n=== Directory Contents ===\")\nif adapter_path.exists():\n    all_files = list(adapter_path.glob(\"*\"))\n    if all_files:\n        print(f\"Files in {adapter_path}:\")\n        for f in sorted(all_files):\n            size_mb = f.stat().st_size / (1024 * 1024)\n            print(f\"  {f.name} - {size_mb:.1f}MB\")\n    else:\n        print(f\"Directory exists but is empty\")\nelse:\n    print(f\"Directory does not exist yet\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation and Testing\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "print(\"=== Model Evaluation Setup ===\")\n",
    "\n",
    "def test_trained_model():\n",
    "    \"\"\"Test the trained model with sample prompts\"\"\"\n",
    "    adapter_path_obj = Path(mlx_config['adapter_path'])\n",
    "    adapter_file = adapter_path_obj / \"adapters.npz\"\n",
    "    \n",
    "    if not adapter_file.exists():\n",
    "        print(f\"‚ùå No trained adapters found at {adapter_file}\")\n",
    "        print(f\"   Complete training first\")\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        from mlx_lm import load, generate\n",
    "        \n",
    "        print(f\"Loading model with adapters...\")\n",
    "        # Load base model with trained adapters\n",
    "        model, tokenizer = load(\n",
    "            TRAINING_MODEL,\n",
    "            adapter_path=str(adapter_path_obj)\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ Model loaded successfully\")\n",
    "        \n",
    "        # Test prompts from different themes\n",
    "        test_prompts = [\n",
    "            \"Give me advice about perseverance\",\n",
    "            \"Share wisdom about leadership\", \n",
    "            \"Tell me about courage\",\n",
    "            \"What do you know about success?\",\n",
    "            \"Give me motivation for hard times\"\n",
    "        ]\n",
    "        \n",
    "        print(f\"\\n=== Model Testing ===\")\n",
    "        for i, prompt in enumerate(test_prompts, 1):\n",
    "            print(f\"\\nTest {i}: {prompt}\")\n",
    "            \n",
    "            try:\n",
    "                response = generate(\n",
    "                    model, tokenizer, \n",
    "                    prompt=prompt,\n",
    "                    max_tokens=100,\n",
    "                    temp=0.7\n",
    "                )\n",
    "                print(f\"Response: {response}\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Generation error: {e}\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except ImportError:\n",
    "        print(f\"‚ùå MLX-LM not available for testing\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Model loading error: {e}\")\n",
    "        return False\n",
    "\n",
    "# Test model if adapters exist\n",
    "adapter_exists = (Path(mlx_config['adapter_path']) / \"adapters.npz\").exists()\n",
    "if adapter_exists:\n",
    "    print(f\"üéØ Testing trained model...\")\n",
    "    test_success = test_trained_model()\n",
    "    if test_success:\n",
    "        print(f\"\\n‚úÖ Model evaluation completed successfully!\")\n",
    "else:\n",
    "    print(f\"‚è≥ Training not completed yet\")\n",
    "    print(f\"   Run this cell again after training finishes\")\n",
    "    print(f\"   Expected file: {Path(mlx_config['adapter_path']) / 'adapters.npz'}\")\n",
    "\n",
    "print(f\"\\n=== Quality Assessment Guidelines ===\")\n",
    "print(f\"Good signs:\")\n",
    "print(f\"  ‚úÖ Responses are coherent and relevant\")\n",
    "print(f\"  ‚úÖ Quotes match the requested theme/topic\")\n",
    "print(f\"  ‚úÖ Output style matches training data\")\n",
    "print(f\"  ‚úÖ No repetitive or nonsensical text\")\n",
    "\n",
    "print(f\"\\nWarning signs:\")\n",
    "print(f\"  ‚ö†Ô∏è  Generic responses unrelated to prompt\")\n",
    "print(f\"  ‚ö†Ô∏è  Repetitive or looping text\")\n",
    "print(f\"  ‚ö†Ô∏è  Factual inaccuracies or hallucinations\")\n",
    "print(f\"  ‚ö†Ô∏è  Inconsistent formatting\")\n",
    "\n",
    "print(f\"\\nüöÄ MLX Fine-tuning setup complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quote-finetuning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}