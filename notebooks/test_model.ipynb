{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Fine-tuned Quotes Model\n",
    "\n",
    "This notebook tests the MLX LoRA fine-tuned Llama 3.2 3B model for motivational quote generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import MLX-LM for inference\n",
    "import mlx_lm\n",
    "import os\n",
    "\n",
    "print(\"Loading fine-tuned model...\")\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the fine-tuned model with LoRA adapters\n",
    "model, tokenizer = mlx_lm.load(\n",
    "    'mlx-community/Llama-3.2-3B-Instruct-4bit',\n",
    "    adapter_path='../models/llama3.2-3b-quotes-lora-mlx'\n",
    ")\n",
    "\n",
    "print(\"âœ… Model loaded successfully!\")\n",
    "print(f\"Model type: {type(model)}\")\n",
    "print(f\"Tokenizer type: {type(tokenizer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test function\nfrom mlx_lm.sample_utils import make_sampler\n\ndef test_quote_generation(prompt, max_tokens=100, temperature=0.7):\n    \"\"\"Generate a motivational quote for the given prompt\"\"\"\n    print(f\"ðŸŽ¯ Prompt: {prompt}\")\n    print(\"ðŸ¤– Generating...\")\n    \n    # Create sampler with temperature\n    sampler = make_sampler(temp=temperature)\n    \n    response = mlx_lm.generate(\n        model, tokenizer, \n        prompt=prompt,\n        max_tokens=max_tokens,\n        sampler=sampler\n    )\n    \n    print(f\"ðŸ’­ Response: {response}\")\n    print(\"-\" * 50)\n    return response"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1: Perseverance\n",
    "test_quote_generation(\"Give me advice about perseverance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2: Self-discipline\n",
    "test_quote_generation(\"Give me advice about self-discipline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 3: Success\n",
    "test_quote_generation(\"Give me advice about success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 4: Leadership\n",
    "test_quote_generation(\"Give me advice about leadership\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 5: Personal Growth\n",
    "test_quote_generation(\"Give me advice about personal growth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load base model for comparison\nprint(\"Loading base model for comparison...\")\nbase_model, base_tokenizer = mlx_lm.load('mlx-community/Llama-3.2-3B-Instruct-4bit')\nprint(\"âœ… Base model loaded!\")\n\ndef compare_models(prompt, temperature=0.7):\n    \"\"\"Simple side-by-side comparison of fine-tuned vs base model\"\"\"\n    sampler = make_sampler(temp=temperature)\n    \n    print(f\"ðŸŽ¯ Prompt: '{prompt}'\\\\n\")\n    \n    # Fine-tuned model\n    ft_response = mlx_lm.generate(\n        model, tokenizer, \n        prompt=prompt, \n        max_tokens=100, \n        sampler=sampler\n    )\n    \n    # Base model  \n    base_response = mlx_lm.generate(\n        base_model, base_tokenizer, \n        prompt=prompt, \n        max_tokens=100, \n        sampler=sampler\n    )\n    \n    # Side by side display\n    print(f\"ðŸ“š Fine-tuned Model:\\\\n{ft_response}\\\\n\")\n    print(f\"ðŸ”§ Base Model:\\\\n{base_response}\\\\n\")\n    print(\"=\" * 80)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Gradio Chat Interface - Fixed\nimport gradio as gr\nfrom mlx_lm.sample_utils import make_sampler\n\n# Global state for model switching\nmodels_loaded = {\"Fine-tuned\": (model, tokenizer), \"Base\": None}\n\ndef load_base_model():\n    \"\"\"Load base model if not already loaded\"\"\"\n    if models_loaded[\"Base\"] is None:\n        print(\"Loading base model...\")\n        base_model, base_tokenizer = mlx_lm.load('mlx-community/Llama-3.2-3B-Instruct-4bit')\n        models_loaded[\"Base\"] = (base_model, base_tokenizer)\n        print(\"âœ… Base model loaded!\")\n\ndef chat_respond(message, history, model_selection, temperature):\n    \"\"\"Generate chat response with model switching\"\"\"\n    # Load base model if needed\n    if model_selection == \"Base\" and models_loaded[\"Base\"] is None:\n        load_base_model()\n    \n    # Get the selected model\n    selected_model, selected_tokenizer = models_loaded[model_selection]\n    \n    # Simple prompt format (not using chat template to avoid issues)\n    prompt = f\"{message}\"\n    \n    # Generate response\n    sampler = make_sampler(temp=temperature)\n    \n    # Use direct generate instead of stream for simplicity\n    try:\n        response = mlx_lm.generate(\n            selected_model, selected_tokenizer, \n            prompt=prompt, \n            max_tokens=150, \n            sampler=sampler\n        )\n        \n        # Clean up the response (remove the original prompt)\n        if prompt in response:\n            response = response.replace(prompt, \"\").strip()\n        \n        return response\n    except Exception as e:\n        return f\"Error: {str(e)}\"\n\n# Create simple Gradio interface\nwith gr.Blocks() as demo:\n    gr.Markdown(\"# ðŸ¤– MLX Quote Chat\")\n    gr.Markdown(\"Compare **Fine-tuned** vs **Base** model responses\")\n    \n    with gr.Row():\n        model_dropdown = gr.Dropdown(\n            choices=[\"Fine-tuned\", \"Base\"],\n            value=\"Fine-tuned\",\n            label=\"Model\"\n        )\n        temperature = gr.Slider(0.1, 1.5, 0.7, label=\"Temperature\")\n    \n    # Simple chat interface\n    with gr.Row():\n        with gr.Column():\n            prompt_input = gr.Textbox(\n                label=\"Your Prompt\",\n                placeholder=\"Give me advice about courage\",\n                lines=2\n            )\n            \n            generate_btn = gr.Button(\"Generate\", variant=\"primary\")\n            \n            response_output = gr.Textbox(\n                label=\"Response\",\n                lines=6,\n                interactive=False\n            )\n    \n    # Examples\n    gr.Examples(\n        examples=[\n            \"Give me advice about perseverance\",\n            \"Give me advice about courage\",\n            \"Give me advice about success\",\n            \"Give me advice about self-discipline\"\n        ],\n        inputs=prompt_input\n    )\n    \n    # Event handler\n    generate_btn.click(\n        fn=chat_respond,\n        inputs=[prompt_input, gr.State([]), model_dropdown, temperature],\n        outputs=response_output\n    )\n    \n    prompt_input.submit(\n        fn=chat_respond,\n        inputs=[prompt_input, gr.State([]), model_dropdown, temperature],\n        outputs=response_output\n    )\n\n# Launch interface\ndemo.launch(share=False, server_port=7861)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}