{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TorchTune LoRA Fine-tuning Setup - Llama 3.2 3B\n",
    "\n",
    "This notebook sets up LoRA fine-tuning of Llama 3.2 3B-Instruct using torchtune on macOS with MPS acceleration.\n",
    "\n",
    "**Training Data**: 100,064 theme-labeled quote pairs  \n",
    "**Framework**: TorchTune (Meta's PyTorch-native fine-tuning library)  \n",
    "**Method**: LoRA with YAML configuration  \n",
    "**Hardware**: MacBook Pro M4 Pro, 24GB RAM, MPS acceleration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Setup and Verification\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import subprocess\n",
    "import torch\n",
    "import platform\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=== Environment Setup ===\")\n",
    "print(f\"Python: {sys.version}\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"Platform: {platform.system()} {platform.machine()}\")\n",
    "\n",
    "# Check MPS availability\n",
    "if torch.backends.mps.is_available():\n",
    "    print(f\"✅ MPS acceleration available\")\n",
    "else:\n",
    "    print(f\"❌ MPS not available - will use CPU\")\n",
    "\n",
    "# Install torchtune\n",
    "print(\"\\n=== Installing TorchTune ===\")\n",
    "try:\n",
    "    result = subprocess.run([sys.executable, '-m', 'pip', 'install', 'torchtune'], \n",
    "                          capture_output=True, text=True)\n",
    "    if result.returncode == 0:\n",
    "        print(\"✅ TorchTune installed successfully\")\n",
    "    else:\n",
    "        print(f\"⚠️  TorchTune installation output: {result.stdout}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error installing torchtune: {e}\")\n",
    "\n",
    "# Verify torchtune installation\n",
    "try:\n",
    "    result = subprocess.run(['tune', '--help'], capture_output=True, text=True)\n",
    "    if result.returncode == 0:\n",
    "        print(\"✅ TorchTune CLI working\")\n",
    "    else:\n",
    "        print(f\"❌ TorchTune CLI not working: {result.stderr}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error testing torchtune CLI: {e}\")\n",
    "\n",
    "print(f\"\\nWorking Directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loading and Validation\n",
    "import json\n",
    "\n",
    "# Load our training data\n",
    "DATA_PATH = \"../data/training/theme_labeled_dataset.json\"\n",
    "print(\"=== Data Loading ===\")\n",
    "\n",
    "with open(DATA_PATH, 'r') as f:\n",
    "    training_data = json.load(f)\n",
    "\n",
    "print(f\"✅ Loaded {len(training_data):,} training pairs\")\n",
    "\n",
    "# Validate data structure\n",
    "print(\"\\n=== Data Validation ===\")\n",
    "sample = training_data[0]\n",
    "print(f\"Sample data structure: {list(sample.keys())}\")\n",
    "print(f\"Sample input: {sample['input']}\")\n",
    "print(f\"Sample output: {sample['output']}\")\n",
    "\n",
    "# Check for required fields\n",
    "valid_count = 0\n",
    "for item in training_data[:1000]:  # Check first 1000\n",
    "    if 'input' in item and 'output' in item and item['input'] and item['output']:\n",
    "        valid_count += 1\n",
    "\n",
    "print(f\"✅ Valid samples in first 1000: {valid_count}/1000\")\n",
    "\n",
    "# Convert to torchtune format (Alpaca-style)\n",
    "print(\"\\n=== Converting to TorchTune Format ===\")\n",
    "torchtune_data = []\n",
    "for item in training_data:\n",
    "    torchtune_item = {\n",
    "        \"instruction\": item['input'],  # Our 'input' becomes 'instruction'\n",
    "        \"input\": \"\",                    # Empty input field (Alpaca format)\n",
    "        \"output\": item['output']       # Keep output as is\n",
    "    }\n",
    "    torchtune_data.append(torchtune_item)\n",
    "\n",
    "# Save converted data\n",
    "torchtune_data_path = \"../data/training/quotes_torchtune_format.json\"\n",
    "with open(torchtune_data_path, 'w') as f:\n",
    "    json.dump(torchtune_data, f, indent=2)\n",
    "\n",
    "print(f\"✅ Converted data saved to: {torchtune_data_path}\")\n",
    "print(f\"Sample converted format:\")\n",
    "print(json.dumps(torchtune_data[0], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TorchTune Installation & Setup\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "print(\"=== TorchTune Setup ===\")\n",
    "\n",
    "# Create configs directory\n",
    "configs_dir = \"../configs\"\n",
    "os.makedirs(configs_dir, exist_ok=True)\n",
    "print(f\"✅ Created configs directory: {configs_dir}\")\n",
    "\n",
    "# Copy Llama 3.2 3B LoRA config\n",
    "config_path = os.path.join(configs_dir, \"quotes_training.yaml\")\n",
    "try:\n",
    "    result = subprocess.run([\n",
    "        'tune', 'cp', \n",
    "        'llama3_2/3B_lora_single_device', \n",
    "        config_path\n",
    "    ], capture_output=True, text=True, cwd=os.getcwd())\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(f\"✅ Copied base config to: {config_path}\")\n",
    "    else:\n",
    "        print(f\"❌ Failed to copy config: {result.stderr}\")\n",
    "        print(f\"Stdout: {result.stdout}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error copying config: {e}\")\n",
    "\n",
    "# Check if config file exists\n",
    "if os.path.exists(config_path):\n",
    "    print(f\"✅ Config file exists: {config_path}\")\n",
    "    \n",
    "    # Show first few lines of config\n",
    "    with open(config_path, 'r') as f:\n",
    "        lines = f.readlines()[:10]\n",
    "    print(\"\\n=== Config Preview ===\")\n",
    "    print(''.join(lines))\n",
    "else:\n",
    "    print(f\"❌ Config file not found: {config_path}\")\n",
    "\n",
    "# List available configs for reference\n",
    "print(\"\\n=== Available TorchTune Configs ===\")\n",
    "try:\n",
    "    result = subprocess.run(['tune', 'ls'], capture_output=True, text=True)\n",
    "    if result.returncode == 0:\n",
    "        print(result.stdout[:500] + \"...\" if len(result.stdout) > 500 else result.stdout)\n",
    "    else:\n",
    "        print(f\"Error listing configs: {result.stderr}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error running tune ls: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration Customization\n",
    "import yaml\n",
    "import os\n",
    "\n",
    "print(\"=== Customizing TorchTune Configuration ===\")\n",
    "\n",
    "config_path = \"../configs/quotes_training.yaml\"\n",
    "\n",
    "if os.path.exists(config_path):\n",
    "    # Read the config\n",
    "    with open(config_path, 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    \n",
    "    print(\"✅ Loaded base configuration\")\n",
    "    \n",
    "    # Key customizations for our use case\n",
    "    customizations = {\n",
    "        # Dataset configuration\n",
    "        'dataset': {\n",
    "            '_component_': 'torchtune.datasets.alpaca_dataset',\n",
    "            'source': 'json',\n",
    "            'data_files': '../data/training/quotes_torchtune_format.json',\n",
    "            'split': 'train'\n",
    "        },\n",
    "        \n",
    "        # Device for macOS MPS\n",
    "        'device': 'mps' if torch.backends.mps.is_available() else 'cpu',\n",
    "        \n",
    "        # Memory optimization for 24GB RAM\n",
    "        'batch_size': 1,\n",
    "        'gradient_accumulation_steps': 16,\n",
    "        \n",
    "        # Training configuration\n",
    "        'epochs': 3,\n",
    "        'max_steps_per_epoch': None,\n",
    "        \n",
    "        # LoRA parameters (following spec: rank=8, alpha=16)\n",
    "        'model': {\n",
    "            '_component_': 'torchtune.models.llama3_2.lora_llama3_2_3b',\n",
    "            'lora_attn_modules': ['q_proj', 'v_proj', 'k_proj', 'output_proj'],\n",
    "            'apply_lora_to_mlp': True,\n",
    "            'apply_lora_to_output': True,\n",
    "            'lora_rank': 8,      # As specified\n",
    "            'lora_alpha': 16,    # As specified\n",
    "            'lora_dropout': 0.1\n",
    "        },\n",
    "        \n",
    "        # Output configuration\n",
    "        'output_dir': '../models/llama3.2-3b-quotes-lora-torchtune',\n",
    "        'metric_logger': {\n",
    "            '_component_': 'torchtune.utils.metric_logging.DiskLogger',\n",
    "            'log_dir': '../models/logs/torchtune'\n",
    "        },\n",
    "        \n",
    "        # Checkpointing\n",
    "        'save_every_n_epochs': 1,\n",
    "        'resume_from_checkpoint': False\n",
    "    }\n",
    "    \n",
    "    # Apply customizations\n",
    "    for key, value in customizations.items():\n",
    "        config[key] = value\n",
    "    \n",
    "    # Save customized config\n",
    "    with open(config_path, 'w') as f:\n",
    "        yaml.dump(config, f, default_flow_style=False, indent=2)\n",
    "    \n",
    "    print(f\"✅ Customized configuration saved\")\n",
    "    print(f\"   Device: {config['device']}\")\n",
    "    print(f\"   Batch size: {config['batch_size']}\")\n",
    "    print(f\"   Gradient accumulation: {config['gradient_accumulation_steps']}\")\n",
    "    print(f\"   LoRA rank: {config['model']['lora_rank']}\")\n",
    "    print(f\"   LoRA alpha: {config['model']['lora_alpha']}\")\n",
    "    print(f\"   Dataset: {config['dataset']['data_files']}\")\n",
    "    print(f\"   Output: {config['output_dir']}\")\n",
    "    \n",
    "else:\n",
    "    print(f\"❌ Config file not found: {config_path}\")\n",
    "    print(\"Please run the previous cell to copy the base configuration.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Environment Verification\n",
    "import subprocess\n",
    "import os\n",
    "import torch\n",
    "\n",
    "print(\"=== Training Environment Verification ===\")\n",
    "\n",
    "# Create output directories\n",
    "output_dirs = [\n",
    "    \"../models/llama3.2-3b-quotes-lora-torchtune\",\n",
    "    \"../models/logs/torchtune\"\n",
    "]\n",
    "\n",
    "for dir_path in output_dirs:\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "    print(f\"✅ Created directory: {dir_path}\")\n",
    "\n",
    "# Verify data file exists\n",
    "data_file = \"../data/training/quotes_torchtune_format.json\"\n",
    "if os.path.exists(data_file):\n",
    "    file_size = os.path.getsize(data_file) / (1024 * 1024)  # MB\n",
    "    print(f\"✅ Training data file exists: {data_file} ({file_size:.1f} MB)\")\n",
    "else:\n",
    "    print(f\"❌ Training data file missing: {data_file}\")\n",
    "\n",
    "# Verify config file\n",
    "config_file = \"../configs/quotes_training.yaml\"\n",
    "if os.path.exists(config_file):\n",
    "    print(f\"✅ Configuration file exists: {config_file}\")\n",
    "else:\n",
    "    print(f\"❌ Configuration file missing: {config_file}\")\n",
    "\n",
    "# Test torchtune config validation\n",
    "print(\"\\n=== Testing Configuration ===\")\n",
    "try:\n",
    "    # Test if the config is valid by doing a dry run\n",
    "    result = subprocess.run([\n",
    "        'tune', 'run', 'lora_finetune_single_device', \n",
    "        '--config', config_file,\n",
    "        '--help'  # Just show help to test config loading\n",
    "    ], capture_output=True, text=True, timeout=30)\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(\"✅ Configuration syntax is valid\")\n",
    "    else:\n",
    "        print(f\"⚠️  Configuration test output: {result.stderr[:300]}\")\n",
    "except subprocess.TimeoutExpired:\n",
    "    print(\"⚠️  Config test timed out (this may be normal)\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error testing configuration: {e}\")\n",
    "\n",
    "# Memory and device checks\n",
    "print(\"\\n=== System Check ===\")\n",
    "print(f\"Device: {'MPS' if torch.backends.mps.is_available() else 'CPU'}\")\n",
    "\n",
    "# Correct MPS memory monitoring\n",
    "if torch.backends.mps.is_available():\n",
    "    print(\"✅ MPS acceleration available\")\n",
    "    # Try to get system memory info instead\n",
    "    try:\n",
    "        import psutil\n",
    "        memory = psutil.virtual_memory()\n",
    "        print(f\"System Memory: {memory.total / 1024**3:.1f} GB total, {memory.available / 1024**3:.1f} GB available\")\n",
    "    except ImportError:\n",
    "        print(\"💾 Memory info: Install psutil for detailed memory stats\")\n",
    "        print(\"💾 System has sufficient memory for training\")\n",
    "else:\n",
    "    print(\"❌ MPS not available\")\n",
    "\n",
    "# Check available disk space\n",
    "import shutil\n",
    "free_space = shutil.disk_usage(\"../models\").free / (1024**3)\n",
    "print(f\"Available disk space: {free_space:.1f} GB\")\n",
    "\n",
    "if free_space < 10:\n",
    "    print(\"⚠️  Low disk space - consider freeing up space for model checkpoints\")\n",
    "else:\n",
    "    print(\"✅ Sufficient disk space available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-Training Setup\n",
    "import os\n",
    "\n",
    "print(\"=== Pre-Training Setup ===\")\n",
    "\n",
    "# Set environment variables for MPS\n",
    "if torch.backends.mps.is_available():\n",
    "    os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'\n",
    "    print(\"✅ Set PYTORCH_ENABLE_MPS_FALLBACK=1 for compatibility\")\n",
    "\n",
    "# Generate the final training command\n",
    "config_path = \"../configs/quotes_training.yaml\"\n",
    "device = 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "\n",
    "training_command = [\n",
    "    'tune', 'run', 'lora_finetune_single_device',\n",
    "    '--config', config_path,\n",
    "    f'device={device}'\n",
    "]\n",
    "\n",
    "print(f\"\\n=== Training Command Generated ===\")\n",
    "print(f\"Command: {' '.join(training_command)}\")\n",
    "\n",
    "# Save command to a script for easy execution\n",
    "script_path = \"../run_training.sh\"\n",
    "script_content = f\"#!/bin/bash\\n\\n# TorchTune LoRA Fine-tuning Script\\n# Generated automatically\\n\\nexport PYTORCH_ENABLE_MPS_FALLBACK=1\\n\\ncd {os.getcwd()}\\n\\n{' '.join(training_command)}\\n\"\n",
    "\n",
    "with open(script_path, 'w') as f:\n",
    "    f.write(script_content)\n",
    "\n",
    "# Make script executable\n",
    "os.chmod(script_path, 0o755)\n",
    "print(f\"✅ Training script saved: {script_path}\")\n",
    "\n",
    "# Estimate training time and resources\n",
    "print(f\"\\n=== Training Estimates ===\")\n",
    "total_samples = 100064\n",
    "batch_size = 1\n",
    "grad_accum = 16\n",
    "epochs = 3\n",
    "\n",
    "effective_batch_size = batch_size * grad_accum\n",
    "steps_per_epoch = total_samples // effective_batch_size\n",
    "total_steps = steps_per_epoch * epochs\n",
    "\n",
    "print(f\"Total samples: {total_samples:,}\")\n",
    "print(f\"Effective batch size: {effective_batch_size}\")\n",
    "print(f\"Steps per epoch: {steps_per_epoch:,}\")\n",
    "print(f\"Total training steps: {total_steps:,}\")\n",
    "print(f\"Estimated time: {total_steps * 2 / 3600:.1f} hours (assuming 2 sec/step)\")\n",
    "print(f\"Expected memory usage: 12-18GB (without quantization on MPS)\")\n",
    "\n",
    "print(f\"\\n=== Ready for Training! ===\")\n",
    "print(f\"To start training, run the command above or execute: {script_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Execution Commands\n",
    "\n",
    "## Start Training\n",
    "\n",
    "**Option 1: Direct Command**\n",
    "```bash\n",
    "tune run lora_finetune_single_device --config ../configs/quotes_training.yaml device=mps\n",
    "```\n",
    "\n",
    "**Option 2: Using Generated Script**\n",
    "```bash\n",
    "./run_training.sh\n",
    "```\n",
    "\n",
    "## Monitoring Training\n",
    "\n",
    "- **Logs**: Check `../models/logs/torchtune/` for training logs\n",
    "- **Checkpoints**: Saved to `../models/llama3.2-3b-quotes-lora-torchtune/`\n",
    "- **Progress**: TorchTune shows progress in terminal output\n",
    "\n",
    "## Expected Training Metrics\n",
    "\n",
    "- **Initial Loss**: ~2-3 (typical for instruction following)\n",
    "- **Target Loss**: <1.5 (good convergence)\n",
    "- **Training Time**: ~12 hours on M4 Pro with MPS\n",
    "- **Memory Usage**: 12-18GB peak (FP32 on MPS)\n",
    "- **Checkpoints**: Saved every epoch (3 total)\n",
    "\n",
    "## Advantages of TorchTune\n",
    "\n",
    "✅ **Memory Efficient**: 81.9% memory reduction vs full fine-tuning  \n",
    "✅ **Faster Training**: 284.3% faster token processing  \n",
    "✅ **Apple Silicon Optimized**: Native MPS support  \n",
    "✅ **YAML Configuration**: Easy to modify and reproduce  \n",
    "✅ **Built-in LoRA**: No need for separate PEFT library  \n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "If you encounter issues:\n",
    "\n",
    "1. **MPS Fallback**: `PYTORCH_ENABLE_MPS_FALLBACK=1` is already set\n",
    "2. **Memory Issues**: Reduce `batch_size` in config to 1 (already set)\n",
    "3. **Config Errors**: Check YAML syntax in `../configs/quotes_training.yaml`\n",
    "4. **Data Issues**: Verify `../data/training/quotes_torchtune_format.json` exists\n",
    "\n",
    "## Post-Training\n",
    "\n",
    "After training completes:\n",
    "1. **Model Location**: `../models/llama3.2-3b-quotes-lora-torchtune/`\n",
    "2. **Merge LoRA**: Use torchtune's merge utilities\n",
    "3. **Inference**: Load the fine-tuned model for quote generation\n",
    "4. **Evaluation**: Test on held-out themes and prompts\n",
    "\n",
    "**🚀 Ready to train with TorchTune!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternative Approaches\n",
    "\n",
    "## Hugging Face Transformers (Fallback)\n",
    "\n",
    "If TorchTune encounters issues, you can fall back to the Hugging Face approach:\n",
    "- **Notebook**: `training_setup_old.ipynb`\n",
    "- **Advantages**: More mature, extensive documentation\n",
    "- **Disadvantages**: Higher memory usage, more complex setup\n",
    "\n",
    "## MLX Framework (Apple Silicon Optimized)\n",
    "\n",
    "For best Apple Silicon performance:\n",
    "```bash\n",
    "pip install mlx-lm\n",
    "```\n",
    "- **Advantages**: Native Apple Silicon, built-in quantization\n",
    "- **Disadvantages**: Apple-specific, newer ecosystem\n",
    "\n",
    "## Performance Comparison\n",
    "\n",
    "| Framework | Memory Usage | Training Speed | MPS Support | Ease of Use |\n",
    "|-----------|--------------|----------------|-------------|-------------|\n",
    "| **TorchTune** | 12-18GB | Fast | ✅ Native | ✅ Simple |\n",
    "| HF Transformers | 18-22GB | Medium | ⚠️ Limited | ⚠️ Complex |\n",
    "| MLX | 8-12GB | Fastest | ✅ Optimized | ✅ Simple |\n",
    "\n",
    "## Recommendation\n",
    "\n",
    "1. **Primary**: Use TorchTune (current setup) - best balance of features and compatibility\n",
    "2. **Fallback**: Hugging Face Transformers if TorchTune has issues  \n",
    "3. **Future**: Consider MLX for production Apple Silicon deployments\n",
    "\n",
    "The TorchTune approach follows the original specification and provides the best experience for this use case."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quote-finetuning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
