{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theme Extraction & Labeling for Quote Fine-tuning\n",
    "\n",
    "This notebook implements theme extraction from motivational quotes using Ollama and generates training data pairs for LLM fine-tuning.\n",
    "\n",
    "**Processing Pipeline:**\n",
    "1. Load unified quotes dataset (33,697 quotes)\n",
    "2. Extract themes using Ollama (llama3.2:latest)\n",
    "3. Generate instruction-output pairs with variations\n",
    "4. Export as TorchTune-compatible JSON\n",
    "5. Generate quality metrics and summary\n",
    "\n",
    "**Target:** 50,000-100,000 training examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import requests\n",
    "import time\n",
    "import logging\n",
    "import os\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "BATCH_SIZE = 500  # Process 500 quotes per batch for memory efficiency\n",
    "OLLAMA_URL = \"http://localhost:11434/api/generate\"\n",
    "OLLAMA_MODEL = \"llama3.2:latest\"\n",
    "MAX_RETRIES = 3\n",
    "TIMEOUT = 30\n",
    "FALLBACK_THEMES = [\"motivation\", \"inspiration\", \"wisdom\"]\n",
    "\n",
    "# Paths\n",
    "DATA_PATH = \"../data/processed/unified_quotes_dataset.csv\"\n",
    "OUTPUT_DIR = \"../data/training\"\n",
    "CACHE_FILE = os.path.join(OUTPUT_DIR, \"theme_extraction_cache.json\")\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Configuration loaded:\")\n",
    "print(f\"- Batch size: {BATCH_SIZE}\")\n",
    "print(f\"- Ollama URL: {OLLAMA_URL}\")\n",
    "print(f\"- Model: {OLLAMA_MODEL}\")\n",
    "print(f\"- Output directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(os.path.join(OUTPUT_DIR, 'theme_extraction.log')),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"Logging configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instruction Template Variations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define instruction template variations\n",
    "INSTRUCTION_TEMPLATES = [\n",
    "    \"Give me a quote about {theme}\",\n",
    "    \"Share wisdom on {theme}\",\n",
    "    \"What's an inspiring quote about {theme}?\",\n",
    "    \"Provide motivation for {theme}\",\n",
    "    \"Give me advice about {theme}\",\n",
    "    \"Share an inspirational message about {theme}\",\n",
    "    \"What would you say to motivate someone about {theme}?\",\n",
    "    \"Inspire me with words about {theme}\"\n",
    "]\n",
    "\n",
    "print(f\"Instruction templates defined ({len(INSTRUCTION_TEMPLATES)} variations):\")\n",
    "for i, template in enumerate(INSTRUCTION_TEMPLATES, 1):\n",
    "    print(f\"  {i}. {template}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading & Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the unified quotes dataset\n",
    "def load_quotes_dataset(file_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load the unified quotes dataset.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        logger.info(f\"Loaded {len(df)} quotes from {file_path}\")\n",
    "        \n",
    "        # Display dataset info\n",
    "        print(f\"\\nDataset Overview:\")\n",
    "        print(f\"- Total quotes: {len(df):,}\")\n",
    "        print(f\"- Columns: {list(df.columns)}\")\n",
    "        print(f\"- Source distribution:\")\n",
    "        print(df['source_dataset'].value_counts().to_string())\n",
    "        print(f\"\\nSample quotes:\")\n",
    "        print(df[['quote_text', 'author', 'source_dataset']].head().to_string())\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading dataset: {e}\")\n",
    "        raise\n",
    "\n",
    "# Load the dataset\n",
    "quotes_df = load_quotes_dataset(DATA_PATH)\n",
    "print(f\"\\nDataset loaded successfully: {len(quotes_df)} quotes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare batches for processing\n",
    "def create_batches(df: pd.DataFrame, batch_size: int) -> List[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Split the DataFrame into batches for processing.\n",
    "    \"\"\"\n",
    "    batches = []\n",
    "    for i in range(0, len(df), batch_size):\n",
    "        batch = df.iloc[i:i+batch_size].copy()\n",
    "        batch.reset_index(drop=True, inplace=True)\n",
    "        batches.append(batch)\n",
    "    \n",
    "    logger.info(f\"Created {len(batches)} batches of size {batch_size}\")\n",
    "    return batches\n",
    "\n",
    "# Create batches\n",
    "quote_batches = create_batches(quotes_df, BATCH_SIZE)\n",
    "print(f\"Created {len(quote_batches)} batches for processing\")\n",
    "print(f\"Batch sizes: {[len(batch) for batch in quote_batches[:5]]}{'...' if len(quote_batches) > 5 else ''}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ollama Integration & Theme Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_ollama_connection() -> bool:\n",
    "    \"\"\"\n",
    "    Test connection to Ollama and verify the model is available.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Test basic connection\n",
    "        response = requests.post(\n",
    "            OLLAMA_URL,\n",
    "            json={\n",
    "                \"model\": OLLAMA_MODEL,\n",
    "                \"prompt\": \"Test connection. Respond with 'OK'.\",\n",
    "                \"stream\": False\n",
    "            },\n",
    "            timeout=10\n",
    "        )\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            result = response.json()\n",
    "            logger.info(f\"Ollama connection successful. Response: {result.get('response', '')[:50]}\")\n",
    "            return True\n",
    "        else:\n",
    "            logger.error(f\"Ollama connection failed. Status: {response.status_code}\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error testing Ollama connection: {e}\")\n",
    "        return False\n",
    "\n",
    "# Test connection\n",
    "if test_ollama_connection():\n",
    "    print(\"âœ… Ollama connection successful\")\n",
    "else:\n",
    "    print(\"âŒ Ollama connection failed - please check if Ollama is running\")\n",
    "    print(\"Run: ollama serve\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_themes_from_quote(quote_text: str, retries: int = MAX_RETRIES) -> List[str]:\n",
    "    \"\"\"\n",
    "    Extract exactly 2 themes from a quote using Ollama.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"Without thinking, analyze this motivational quote below and extract exactly 2 core themes. Return only the themes as a comma-separated list and nothing else.\n",
    "\n",
    "Quote: \"{quote_text}\"\n",
    "\n",
    "Return only the themes.\"\"\"\n",
    "    \n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            response = requests.post(\n",
    "                OLLAMA_URL,\n",
    "                json={\n",
    "                    \"model\": OLLAMA_MODEL,\n",
    "                    \"prompt\": prompt,\n",
    "                    \"stream\": False\n",
    "                },\n",
    "                timeout=TIMEOUT\n",
    "            )\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                result = response.json()\n",
    "                themes_text = result.get('response', '').strip()\n",
    "                \n",
    "                # Parse themes from response\n",
    "                themes = parse_themes_response(themes_text)\n",
    "                \n",
    "                if themes:\n",
    "                    return themes\n",
    "                    \n",
    "            logger.warning(f\"Attempt {attempt + 1} failed for quote: {quote_text[:50]}...\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Attempt {attempt + 1} error: {e}\")\n",
    "            \n",
    "        if attempt < retries - 1:\n",
    "            time.sleep(1)  # Brief delay before retry\n",
    "    \n",
    "    # Return fallback themes if all attempts failed\n",
    "    logger.error(f\"All attempts failed for quote: {quote_text[:50]}... Using fallback themes.\")\n",
    "    return FALLBACK_THEMES[:2]  # Return 2 fallback themes\n",
    "\n",
    "\n",
    "def parse_themes_response(response_text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Parse themes from Ollama response text, limiting to exactly 2 themes.\n",
    "    \"\"\"\n",
    "    if not response_text:\n",
    "        return []\n",
    "    \n",
    "    # Clean the response\n",
    "    response_text = response_text.strip()\n",
    "    \n",
    "    # Split by comma and clean each theme\n",
    "    themes = []\n",
    "    for theme in response_text.split(','):\n",
    "        theme = theme.strip().lower()\n",
    "        # Remove quotes, periods, and extra whitespace\n",
    "        theme = re.sub(r'^[\"\\']|[\"\\']$', '', theme)\n",
    "        theme = re.sub(r'[.!?]$', '', theme)\n",
    "        theme = re.sub(r'\\s+', ' ', theme)\n",
    "        \n",
    "        # Filter out themes with more than 10 words (conversational responses)\n",
    "        word_count = len(theme.split())\n",
    "        \n",
    "        if theme and len(theme) > 2 and word_count <= 10:  # Valid theme: >2 chars, â‰¤10 words\n",
    "            themes.append(theme)\n",
    "            \n",
    "        if len(themes) >= 2:  # Limit to exactly 2 themes\n",
    "            break\n",
    "    \n",
    "    return themes\n",
    "\n",
    "# Test theme extraction with a sample quote\n",
    "sample_quote = \"The only impossible journey is the one you never begin.\"\n",
    "sample_themes = extract_themes_from_quote(sample_quote)\n",
    "print(f\"Sample extraction:\")\n",
    "print(f\"Quote: {sample_quote}\")\n",
    "print(f\"Themes: {sample_themes}\")\n",
    "print(f\"Number of themes: {len(sample_themes)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Processing with Caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cache() -> Dict[str, List[str]]:\n",
    "    \"\"\"\n",
    "    Load theme extraction cache to avoid reprocessing.\n",
    "    \"\"\"\n",
    "    if os.path.exists(CACHE_FILE):\n",
    "        try:\n",
    "            with open(CACHE_FILE, 'r') as f:\n",
    "                cache = json.load(f)\n",
    "            logger.info(f\"Loaded cache with {len(cache)} entries\")\n",
    "            return cache\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error loading cache: {e}\")\n",
    "    \n",
    "    return {}\n",
    "\n",
    "\n",
    "def save_cache(cache: Dict[str, List[str]]) -> None:\n",
    "    \"\"\"\n",
    "    Save theme extraction cache.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(CACHE_FILE, 'w') as f:\n",
    "            json.dump(cache, f, indent=2)\n",
    "        logger.info(f\"Saved cache with {len(cache)} entries\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error saving cache: {e}\")\n",
    "\n",
    "\n",
    "def process_batch(batch_df: pd.DataFrame, cache: Dict[str, List[str]], batch_num: int) -> Dict[str, List[str]]:\n",
    "    \"\"\"\n",
    "    Process a batch of quotes for theme extraction.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Processing batch {batch_num} with {len(batch_df)} quotes\")\n",
    "    \n",
    "    batch_cache = cache.copy()\n",
    "    processed_count = 0\n",
    "    cached_count = 0\n",
    "    \n",
    "    for idx, row in tqdm(batch_df.iterrows(), total=len(batch_df), \n",
    "                        desc=f\"Batch {batch_num}\", leave=False):\n",
    "        quote_text = row['quote_text']\n",
    "        \n",
    "        # Check cache first\n",
    "        if quote_text in batch_cache:\n",
    "            cached_count += 1\n",
    "            continue\n",
    "        \n",
    "        # Extract themes\n",
    "        themes = extract_themes_from_quote(quote_text)\n",
    "        batch_cache[quote_text] = themes\n",
    "        processed_count += 1\n",
    "        \n",
    "        # Brief pause to avoid overwhelming Ollama\n",
    "        time.sleep(0.1)\n",
    "    \n",
    "    logger.info(f\"Batch {batch_num} complete: {processed_count} processed, {cached_count} cached\")\n",
    "    return batch_cache\n",
    "\n",
    "# Load existing cache\n",
    "theme_cache = load_cache()\n",
    "print(f\"Loaded cache with {len(theme_cache)} existing entries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all batches\n",
    "def process_all_batches(batches: List[pd.DataFrame], cache: Dict[str, List[str]]) -> Dict[str, List[str]]:\n",
    "    \"\"\"\n",
    "    Process all batches for theme extraction.\n",
    "    \"\"\"\n",
    "    total_batches = len(batches)\n",
    "    start_time = time.time()\n",
    "    \n",
    "    logger.info(f\"Starting processing of {total_batches} batches\")\n",
    "    \n",
    "    for batch_num, batch_df in enumerate(tqdm(batches, desc=\"Processing batches\"), 1):\n",
    "        cache = process_batch(batch_df, cache, batch_num)\n",
    "        \n",
    "        # Save cache after each batch\n",
    "        save_cache(cache)\n",
    "        logger.info(f\"Saved cache after batch {batch_num}\")\n",
    "    \n",
    "    # Final cache save\n",
    "    save_cache(cache)\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    logger.info(f\"All batches processed in {elapsed_time:.2f} seconds\")\n",
    "    logger.info(f\"Final cache size: {len(cache)} entries\")\n",
    "    \n",
    "    return cache\n",
    "\n",
    "# Process all batches\n",
    "print(f\"\\nStarting theme extraction for {len(quote_batches)} batches...\")\n",
    "print(f\"This may take 1-2 hours for the full dataset\")\n",
    "\n",
    "theme_cache = process_all_batches(quote_batches, theme_cache)\n",
    "print(f\"\\nâœ… Theme extraction complete!\")\n",
    "print(f\"Total themes extracted: {len(theme_cache)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_pairs(quote_text: str, themes: List[str]) -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Generate training pairs for a quote and its themes.\n",
    "    Now uses 1-2 random instruction templates per theme for more manageable dataset size.\n",
    "    \"\"\"\n",
    "    pairs = []\n",
    "    \n",
    "    for theme in themes:\n",
    "        # Select 1-2 random instruction templates per theme (instead of 2-3)\n",
    "        import random\n",
    "        num_templates = random.choice([1, 2])  # Randomly choose 1 or 2 templates\n",
    "        selected_templates = random.sample(INSTRUCTION_TEMPLATES, \n",
    "                                         min(num_templates, len(INSTRUCTION_TEMPLATES)))\n",
    "        \n",
    "        for template in selected_templates:\n",
    "            instruction = template.format(theme=theme)\n",
    "            \n",
    "            pair = {\n",
    "                \"input\": instruction,\n",
    "                \"output\": quote_text\n",
    "            }\n",
    "            pairs.append(pair)\n",
    "    \n",
    "    return pairs\n",
    "\n",
    "\n",
    "def create_training_dataset(quotes_df: pd.DataFrame, theme_cache: Dict[str, List[str]]) -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Create the complete training dataset.\n",
    "    \"\"\"\n",
    "    training_pairs = []\n",
    "    quotes_processed = 0\n",
    "    quotes_skipped = 0\n",
    "    \n",
    "    logger.info(\"Generating training pairs...\")\n",
    "    \n",
    "    for idx, row in tqdm(quotes_df.iterrows(), total=len(quotes_df), desc=\"Creating training pairs\"):\n",
    "        quote_text = row['quote_text']\n",
    "        \n",
    "        # Get themes from cache\n",
    "        if quote_text in theme_cache:\n",
    "            themes = theme_cache[quote_text]\n",
    "            \n",
    "            if themes:  # Only process if themes were extracted\n",
    "                pairs = generate_training_pairs(quote_text, themes)\n",
    "                training_pairs.extend(pairs)\n",
    "                quotes_processed += 1\n",
    "            else:\n",
    "                quotes_skipped += 1\n",
    "        else:\n",
    "            quotes_skipped += 1\n",
    "    \n",
    "    logger.info(f\"Training dataset created:\")\n",
    "    logger.info(f\"- Quotes processed: {quotes_processed}\")\n",
    "    logger.info(f\"- Quotes skipped: {quotes_skipped}\")\n",
    "    logger.info(f\"- Training pairs generated: {len(training_pairs)}\")\n",
    "    \n",
    "    return training_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate training dataset from current cache (first batch processed)\n",
    "print(\"ðŸ“š Generating training dataset from current cache...\")\n",
    "\n",
    "# Load current cache\n",
    "theme_cache = load_cache()\n",
    "print(f\"ðŸ“Š Cache loaded: {len(theme_cache)} quotes processed ({len(theme_cache)/len(quotes_df)*100:.1f}% of dataset)\")\n",
    "\n",
    "# Create training dataset using the current theme cache\n",
    "# Note: This will only process quotes that have themes in the cache\n",
    "training_dataset = create_training_dataset(quotes_df, theme_cache)\n",
    "\n",
    "print(f\"âœ… Training dataset created with {len(training_dataset)} pairs\")\n",
    "print(f\"Average pairs per processed quote: {len(training_dataset)/len(theme_cache):.1f}\")\n",
    "\n",
    "# Show sample training pairs to verify the optimized format\n",
    "print(\"\\nðŸ” Sample training pairs (showing optimized 2-theme system):\")\n",
    "for i, pair in enumerate(training_dataset[:8], 1):\n",
    "    print(f\"\\n{i}. Input: {pair['input']}\")\n",
    "    print(f\"   Output: {pair['output'][:80]}{'...' if len(pair['output']) > 80 else ''}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate training dataset from current cache\n",
    "print(\"ðŸ“š Generating training dataset from current cache...\")\n",
    "\n",
    "# Create training dataset using the current theme cache\n",
    "training_dataset = create_training_dataset(quotes_df, theme_cache)\n",
    "\n",
    "print(f\"âœ… Training dataset created with {len(training_dataset)} pairs\")\n",
    "\n",
    "# Show sample training pairs\n",
    "print(\"\\nSample training pairs:\")\n",
    "for i, pair in enumerate(training_dataset[:10], 1):\n",
    "    print(f\"\\n{i}. Input: {pair['input']}\")\n",
    "    print(f\"   Output: {pair['output']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export & Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_training_data(training_data: List[Dict[str, str]]) -> Dict[str, any]:\n",
    "    \"\"\"\n",
    "    Validate the training dataset and return quality metrics.\n",
    "    \"\"\"\n",
    "    if not training_data:\n",
    "        return {\"valid\": False, \"error\": \"Empty dataset\"}\n",
    "    \n",
    "    # Validation checks\n",
    "    valid_pairs = 0\n",
    "    input_lengths = []\n",
    "    output_lengths = []\n",
    "    duplicate_pairs = set()\n",
    "    \n",
    "    for pair in training_data:\n",
    "        # Check required keys\n",
    "        if \"input\" not in pair or \"output\" not in pair:\n",
    "            continue\n",
    "            \n",
    "        # Check for non-empty values\n",
    "        if not pair[\"input\"] or not pair[\"output\"]:\n",
    "            continue\n",
    "            \n",
    "        valid_pairs += 1\n",
    "        input_lengths.append(len(pair[\"input\"]))\n",
    "        output_lengths.append(len(pair[\"output\"]))\n",
    "        \n",
    "        # Check for duplicates\n",
    "        pair_signature = f\"{pair['input']}|{pair['output']}\"\n",
    "        if pair_signature in duplicate_pairs:\n",
    "            continue\n",
    "        duplicate_pairs.add(pair_signature)\n",
    "    \n",
    "    metrics = {\n",
    "        \"valid\": True,\n",
    "        \"total_pairs\": len(training_data),\n",
    "        \"valid_pairs\": valid_pairs,\n",
    "        \"unique_pairs\": len(duplicate_pairs),\n",
    "        \"avg_input_length\": sum(input_lengths) / len(input_lengths) if input_lengths else 0,\n",
    "        \"avg_output_length\": sum(output_lengths) / len(output_lengths) if output_lengths else 0,\n",
    "        \"min_input_length\": min(input_lengths) if input_lengths else 0,\n",
    "        \"max_input_length\": max(input_lengths) if input_lengths else 0,\n",
    "        \"min_output_length\": min(output_lengths) if output_lengths else 0,\n",
    "        \"max_output_length\": max(output_lengths) if output_lengths else 0\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "def export_training_data(training_data: List[Dict[str, str]], output_path: str) -> bool:\n",
    "    \"\"\"\n",
    "    Export training data as TorchTune-compatible JSON.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create backup\n",
    "        backup_path = output_path.replace('.json', '_backup.json')\n",
    "        \n",
    "        with open(output_path, 'w') as f:\n",
    "            json.dump(training_data, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        with open(backup_path, 'w') as f:\n",
    "            json.dump(training_data, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        logger.info(f\"Training data exported to {output_path}\")\n",
    "        logger.info(f\"Backup created at {backup_path}\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error exporting training data: {e}\")\n",
    "        return False\n",
    "\n",
    "# Validate training data\n",
    "print(\"\\nValidating training dataset...\")\n",
    "validation_results = validate_training_data(training_dataset)\n",
    "\n",
    "if validation_results[\"valid\"]:\n",
    "    print(\"âœ… Validation passed!\")\n",
    "    print(f\"\\nValidation Results:\")\n",
    "    for key, value in validation_results.items():\n",
    "        if key != \"valid\":\n",
    "            print(f\"- {key.replace('_', ' ').title()}: {value:,.2f}\" if isinstance(value, float) else f\"- {key.replace('_', ' ').title()}: {value:,}\")\n",
    "else:\n",
    "    print(f\"âŒ Validation failed: {validation_results.get('error', 'Unknown error')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export training data\n",
    "output_file = os.path.join(OUTPUT_DIR, \"theme_labeled_dataset.json\")\n",
    "\n",
    "print(f\"\\nExporting training dataset...\")\n",
    "if export_training_data(training_dataset, output_file):\n",
    "    print(f\"âœ… Export successful!\")\n",
    "    print(f\"Training data saved to: {output_file}\")\n",
    "    \n",
    "    # Show file size\n",
    "    file_size = os.path.getsize(output_file) / (1024 * 1024)  # MB\n",
    "    print(f\"File size: {file_size:.1f} MB\")\n",
    "else:\n",
    "    print(\"âŒ Export failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quality Metrics & Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_theme_distribution(theme_cache: Dict[str, List[str]]) -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    Analyze the distribution of extracted themes.\n",
    "    \"\"\"\n",
    "    theme_counts = {}\n",
    "    \n",
    "    for themes in theme_cache.values():\n",
    "        for theme in themes:\n",
    "            theme_counts[theme] = theme_counts.get(theme, 0) + 1\n",
    "    \n",
    "    # Sort by frequency\n",
    "    sorted_themes = dict(sorted(theme_counts.items(), key=lambda x: x[1], reverse=True))\n",
    "    \n",
    "    return sorted_themes\n",
    "\n",
    "\n",
    "def generate_summary_report(quotes_df: pd.DataFrame, theme_cache: Dict[str, List[str]], \n",
    "                           training_data: List[Dict[str, str]], validation_results: Dict[str, any]) -> Dict[str, any]:\n",
    "    \"\"\"\n",
    "    Generate comprehensive summary report.\n",
    "    \"\"\"\n",
    "    theme_distribution = analyze_theme_distribution(theme_cache)\n",
    "    \n",
    "    # Calculate success rates\n",
    "    total_quotes = len(quotes_df)\n",
    "    processed_quotes = len(theme_cache)\n",
    "    success_rate = (processed_quotes / total_quotes) * 100 if total_quotes > 0 else 0\n",
    "    \n",
    "    # Theme statistics\n",
    "    themes_per_quote = [len(themes) for themes in theme_cache.values() if themes]\n",
    "    avg_themes_per_quote = sum(themes_per_quote) / len(themes_per_quote) if themes_per_quote else 0\n",
    "    \n",
    "    # Instruction template usage\n",
    "    template_usage = {}\n",
    "    for pair in training_data:\n",
    "        input_text = pair[\"input\"]\n",
    "        for i, template in enumerate(INSTRUCTION_TEMPLATES):\n",
    "            # Extract the theme part to check template usage\n",
    "            template_pattern = template.replace(\"{theme}\", \".*\")\n",
    "            if re.match(template_pattern, input_text):\n",
    "                template_usage[f\"Template {i+1}\"] = template_usage.get(f\"Template {i+1}\", 0) + 1\n",
    "                break\n",
    "    \n",
    "    report = {\n",
    "        \"processing_summary\": {\n",
    "            \"total_quotes\": total_quotes,\n",
    "            \"quotes_processed\": processed_quotes,\n",
    "            \"success_rate_percent\": round(success_rate, 2),\n",
    "            \"processing_timestamp\": datetime.now().isoformat()\n",
    "        },\n",
    "        \"theme_extraction\": {\n",
    "            \"unique_themes_extracted\": len(theme_distribution),\n",
    "            \"avg_themes_per_quote\": round(avg_themes_per_quote, 2),\n",
    "            \"top_10_themes\": dict(list(theme_distribution.items())[:10]),\n",
    "            \"theme_distribution_stats\": {\n",
    "                \"most_common\": max(theme_distribution.values()) if theme_distribution else 0,\n",
    "                \"least_common\": min(theme_distribution.values()) if theme_distribution else 0,\n",
    "                \"median_frequency\": sorted(theme_distribution.values())[len(theme_distribution)//2] if theme_distribution else 0\n",
    "            }\n",
    "        },\n",
    "        \"training_data\": {\n",
    "            \"total_training_pairs\": len(training_data),\n",
    "            \"pairs_per_quote_avg\": round(len(training_data) / total_quotes, 2) if total_quotes > 0 else 0,\n",
    "            \"validation_results\": validation_results,\n",
    "            \"template_usage\": template_usage\n",
    "        },\n",
    "        \"file_outputs\": {\n",
    "            \"training_dataset\": output_file,\n",
    "            \"cache_file\": CACHE_FILE,\n",
    "            \"log_file\": os.path.join(OUTPUT_DIR, 'theme_extraction.log')\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return report\n",
    "\n",
    "# Generate summary report\n",
    "print(\"\\nGenerating summary report...\")\n",
    "summary_report = generate_summary_report(quotes_df, theme_cache, training_dataset, validation_results)\n",
    "\n",
    "# Save summary report\n",
    "summary_file = os.path.join(OUTPUT_DIR, \"theme_extraction_summary.json\")\n",
    "with open(summary_file, 'w') as f:\n",
    "    json.dump(summary_report, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"âœ… Summary report saved to: {summary_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
