adapter_path: /Users/thomaybalazs/Projects/quotes-finetuning/models/llama3.2-3b-quotes-lora-mlx
batch_size: 2
data: /Users/thomaybalazs/Projects/quotes-finetuning/data/training/mlx_format
grad_checkpoint: true
iters: 2000
learning_rate: 5.0e-05
lora_layers: 16
lora_parameters:
  dropout: 0.1
  keys:
  - self_attn.q_proj
  - self_attn.v_proj
  - self_attn.k_proj
  - self_attn.o_proj
  rank: 8
  scale: 16.0
max_seq_length: 2048
model: /Users/thomaybalazs/Projects/quotes-finetuning/notebooks/mlx-community/Llama-3.2-3B-Instruct-4bit
save_every: 200
seed: 42
steps_per_eval: 100
steps_per_report: 25
train: true
val_batches: 25
